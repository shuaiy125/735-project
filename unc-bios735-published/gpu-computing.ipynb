{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "784103ca",
      "metadata": {
        "id": "784103ca"
      },
      "source": [
        "# GPU Computing\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The previous lecture covered techniques for writing efficient Python code: profiling, vectorization, Numba JIT compilation, and Cython. These approaches optimize code running on the CPU. For certain computations, we can achieve even greater speedups by using a Graphics Processing Unit (GPU).\n",
        "\n",
        "GPUs were originally designed for rendering graphics, which requires performing the same operation on millions of pixels simultaneously. This architecture turns out to be ideal for many scientific computing tasks: matrix operations, Monte Carlo simulations, and any computation where the same operation is applied to large amounts of data.\n",
        "\n",
        "This lecture covers GPU computing in Python using two libraries:\n",
        "\n",
        "* **CuPy**: A drop-in replacement for NumPy that runs on NVIDIA GPUs\n",
        "* **PyTorch**: A deep learning framework with powerful GPU array operations\n",
        "\n",
        "Both libraries let you write Python code that executes on the GPU with minimal changes to your existing NumPy-based code.\n",
        "\n",
        "### CPU vs GPU Architecture\n",
        "\n",
        "A CPU is designed for general-purpose computing. It has a small number of powerful cores (typically 4-16 on consumer hardware) optimized for complex tasks with branching logic, sequential dependencies, and varied workloads. Each core can handle sophisticated operations independently.\n",
        "\n",
        "A GPU takes a different approach. It has thousands of simpler cores designed to execute the same instruction on many data elements simultaneously. An NVIDIA GPU might have 5,000+ CUDA cores, each less powerful than a CPU core but collectively capable of massive parallelism.\n",
        "\n",
        "```\n",
        "CPU: Few powerful cores          GPU: Many simple cores\n",
        "+----+----+----+----+           +--+--+--+--+--+--+--+--+\n",
        "|    |    |    |    |           |  |  |  |  |  |  |  |  |\n",
        "|Core|Core|Core|Core|           +--+--+--+--+--+--+--+--+\n",
        "|  1 |  2 |  3 |  4 |           |  |  |  |  |  |  |  |  |\n",
        "|    |    |    |    |           +--+--+--+--+--+--+--+--+\n",
        "+----+----+----+----+           |  |  |  |  |  |  |  |  |\n",
        "  4 cores @ 4 GHz               +--+--+--+--+--+--+--+--+\n",
        "                                  ... thousands of cores\n",
        "```\n",
        "\n",
        "This architecture difference means:\n",
        "\n",
        "* **GPUs excel at data parallelism**: When you need to perform the same operation on millions of elements (matrix multiplication, element-wise operations, convolutions), GPUs can process many elements simultaneously.\n",
        "\n",
        "* **CPUs excel at task parallelism and complex logic**: When operations have dependencies, require branching, or involve complex control flow, CPUs are more efficient.\n",
        "\n",
        "### When to Use GPU Computing\n",
        "\n",
        "GPU acceleration provides significant speedups for:\n",
        "\n",
        "* Large matrix operations (multiplication, decomposition)\n",
        "* Element-wise operations on large arrays\n",
        "* Monte Carlo simulations with independent trials\n",
        "* Convolutions and signal processing\n",
        "* Operations where the same computation is applied to many data points\n",
        "\n",
        "GPU acceleration is **not** beneficial for:\n",
        "\n",
        "* Small data (overhead of data transfer exceeds computation time)\n",
        "* Sequential algorithms where each step depends on the previous\n",
        "* Operations with complex branching or conditionals\n",
        "* I/O-bound tasks (reading files, network operations)\n",
        "\n",
        "A rule of thumb: if your computation is embarrassingly parallel and operates on millions of elements, consider GPU acceleration.\n",
        "\n",
        "### The GPU Computing Landscape in Python\n",
        "\n",
        "Several libraries provide GPU computing in Python:\n",
        "\n",
        "* **CuPy**: NumPy-compatible array library for NVIDIA GPUs. Minimal code changes required.\n",
        "* **PyTorch**: Deep learning framework with GPU tensors. Popular for ML but useful for general GPU computing.\n",
        "* **JAX**: Google's library for high-performance numerical computing with automatic differentiation.\n",
        "* **Numba CUDA**: Write custom GPU kernels in Python (covered in efficient code lecture's Numba section).\n",
        "\n",
        "This lecture focuses on CuPy and PyTorch as they cover most statistical computing needs with minimal learning curve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496ae490",
      "metadata": {
        "id": "496ae490"
      },
      "source": [
        "## GPU Architecture Basics\n",
        "\n",
        "Before writing GPU code, understanding a few architectural concepts helps explain performance characteristics.\n",
        "\n",
        "### CUDA and GPU Execution\n",
        "\n",
        "NVIDIA GPUs use CUDA (Compute Unified Device Architecture) as their parallel computing platform. When you run CuPy or PyTorch code on an NVIDIA GPU, it uses CUDA under the hood.\n",
        "\n",
        "Key concepts:\n",
        "\n",
        "* **CUDA cores**: The basic processing units that execute computations in parallel\n",
        "* **Streaming Multiprocessors (SMs)**: Groups of CUDA cores that share resources\n",
        "* **Warps**: Groups of 32 threads that execute instructions in lockstep\n",
        "\n",
        "For practical purposes, you can think of the GPU as having thousands of workers that can all perform the same operation simultaneously on different data.\n",
        "\n",
        "### Host and Device Memory\n",
        "\n",
        "A critical concept in GPU computing is the separation between:\n",
        "\n",
        "* **Host**: The CPU and its main memory (RAM)\n",
        "* **Device**: The GPU and its dedicated memory (VRAM)\n",
        "\n",
        "Data must be explicitly transferred between host and device:\n",
        "\n",
        "```\n",
        "+------------------+      Transfer      +------------------+\n",
        "|      HOST        |  <------------>   |     DEVICE       |\n",
        "|   CPU + RAM      |                   |   GPU + VRAM     |\n",
        "| (numpy arrays)   |                   | (cupy/torch)     |\n",
        "+------------------+                   +------------------+\n",
        "```\n",
        "\n",
        "This data transfer has overhead. Moving a large array to the GPU takes time, even before any computation begins. The key to efficient GPU programming is:\n",
        "\n",
        "1. Transfer data to GPU once\n",
        "2. Perform many operations on GPU\n",
        "3. Transfer results back when done\n",
        "\n",
        "Avoid repeatedly moving data back and forth between CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cupy-cuda12x\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "tSrIOW18tt4s"
      },
      "id": "tSrIOW18tt4s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLhSCcd5vRL-"
      },
      "id": "MLhSCcd5vRL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking GPU Availability\n",
        "\n",
        "Before running GPU code, verify that a GPU is available:"
      ],
      "metadata": {
        "id": "Arkv9M_Qtnmv"
      },
      "id": "Arkv9M_Qtnmv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e393524",
      "metadata": {
        "id": "0e393524"
      },
      "outputs": [],
      "source": [
        "# Check with CuPy\n",
        "import cupy as cp\n",
        "\n",
        "print(f\"CuPy version: {cp.__version__}\")\n",
        "print(f\"CUDA available: {cp.cuda.is_available()}\")\n",
        "if cp.cuda.is_available():\n",
        "    print(f\"GPU device: {cp.cuda.runtime.getDeviceProperties(0)['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc56983",
      "metadata": {
        "id": "bbc56983"
      },
      "outputs": [],
      "source": [
        "# Check with PyTorch\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab097de",
      "metadata": {
        "id": "9ab097de"
      },
      "source": [
        "### Question\n",
        "\n",
        "Consider these three computational tasks:\n",
        "\n",
        "1. Computing the mean of 100 numbers\n",
        "2. Multiplying two 10000x10000 matrices\n",
        "3. A recursive Fibonacci calculation\n",
        "\n",
        "Which task(s) would benefit from GPU acceleration, and why?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Only task 2 (matrix multiplication) would benefit from GPU acceleration.\n",
        "\n",
        "* **Task 1** involves too little data. The overhead of transferring 100 numbers to the GPU would exceed the computation time.\n",
        "* **Task 2** is ideal for GPU: matrix multiplication is highly parallelizable, and the large size (100 million operations) amortizes the transfer overhead.\n",
        "* **Task 3** is inherently sequential. Each Fibonacci number depends on the previous two, so there's no parallelism to exploit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057d1eb2",
      "metadata": {
        "id": "057d1eb2"
      },
      "source": [
        "## CuPy: NumPy on the GPU\n",
        "\n",
        "CuPy provides a NumPy-compatible interface for GPU arrays. Most NumPy code can run on GPU by simply replacing `numpy` with `cupy`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d7e27a",
      "metadata": {
        "id": "56d7e27a"
      },
      "source": [
        "### Basic Usage\n",
        "\n",
        "CuPy arrays work like NumPy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b523dbdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b523dbdf",
        "outputId": "3d538056-ba72-4627-be06-8dff0d24d518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 10,000,000 random numbers on CPU...\n",
            "--- CuPy (GPU) ---\n",
            "GPU Execution Time: 1.6526 ms\n",
            "\n",
            "--- NumPy (CPU) ---\n",
            "CPU Execution Time: 34.6861 ms\n",
            "\n",
            "Speedup: 20.99x\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "# Generate large arrays on CPU first to ensure same values\n",
        "N = 10_000_000\n",
        "print(f\"Generating {N:,} random numbers on CPU...\")\n",
        "a_cpu = np.random.rand(N).astype(np.float32)\n",
        "b_cpu = np.random.rand(N).astype(np.float32)\n",
        "\n",
        "print(\"--- CuPy (GPU) ---\")\n",
        "# Transfer to GPU before timing computation to isolate processing speed\n",
        "a_gpu = cp.asarray(a_cpu)\n",
        "b_gpu = cp.asarray(b_cpu)\n",
        "\n",
        "# Synchronize before timing to ensure GPU is ready\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "start_gpu = time.perf_counter()\n",
        "\n",
        "# Operations run on GPU\n",
        "c_gpu = a_gpu + b_gpu\n",
        "d_gpu = cp.sqrt(a_gpu)\n",
        "e_gpu = cp.dot(a_gpu, b_gpu)\n",
        "\n",
        "# Synchronize after operations to wait for completion\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "end_gpu = time.perf_counter()\n",
        "\n",
        "print(f\"GPU Execution Time: {(end_gpu - start_gpu) * 1000:.4f} ms\")\n",
        "\n",
        "print(\"\\n--- NumPy (CPU) ---\")\n",
        "start_cpu = time.perf_counter()\n",
        "\n",
        "# Operations run on CPU\n",
        "c_cpu = a_cpu + b_cpu\n",
        "d_cpu = np.sqrt(a_cpu)\n",
        "e_cpu = np.dot(a_cpu, b_cpu)\n",
        "\n",
        "end_cpu = time.perf_counter()\n",
        "\n",
        "print(f\"CPU Execution Time: {(end_cpu - start_cpu) * 1000:.4f} ms\")\n",
        "\n",
        "speedup = (end_cpu - start_cpu) / (end_gpu - start_gpu)\n",
        "print(f\"\\nSpeedup: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15986f8",
      "metadata": {
        "id": "b15986f8"
      },
      "source": [
        "### Transferring Data Between CPU and GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a72eb61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a72eb61",
        "outputId": "e4ecdf45-564f-41c8-8768-508b2e3548f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type on GPU: <class 'cupy.ndarray'>\n",
            "Result: 1.0010916407329538\n",
            "Type on CPU: <class 'numpy.ndarray'>\n",
            "Result: 1.0010916407329538\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "# Create NumPy array on CPU\n",
        "data_cpu = np.random.randn(1000000)\n",
        "\n",
        "# Transfer to GPU\n",
        "data_gpu = cp.asarray(data_cpu)\n",
        "\n",
        "# Perform computation on GPU\n",
        "result_gpu = cp.mean(data_gpu ** 2)\n",
        "\n",
        "# Transfer result back to CPU\n",
        "result_cpu = result_gpu.get()  # or: cp.asnumpy(result_gpu)\n",
        "\n",
        "print(f\"Type on GPU: {type(result_gpu)}\")\n",
        "print(f\"Result: {result_gpu}\")\n",
        "print(f\"Type on CPU: {type(result_cpu)}\")\n",
        "print(f\"Result: {result_cpu}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e8040a",
      "metadata": {
        "id": "49e8040a"
      },
      "source": [
        "Key functions:\n",
        "* `cp.asarray(numpy_array)`: Transfer NumPy array to GPU\n",
        "* `gpu_array.get()` or `cp.asnumpy(gpu_array)`: Transfer CuPy array to CPU\n",
        "\n",
        "### CuPy as a Drop-in Replacement\n",
        "\n",
        "Most NumPy functions have CuPy equivalents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b040e37d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b040e37d",
        "outputId": "760e4f59-4733-4ace-e191-a1db2496e596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy result: 9999999.99999999\n",
            "CuPy result: 10000000.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "# NumPy code\n",
        "def compute_stats_numpy(data):\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    normalized = (data - mean) / std\n",
        "    return np.sum(normalized ** 2)\n",
        "\n",
        "# CuPy code - just change np to cp\n",
        "def compute_stats_cupy(data):\n",
        "    mean = cp.mean(data)\n",
        "    std = cp.std(data)\n",
        "    normalized = (data - mean) / std\n",
        "    return cp.sum(normalized ** 2)\n",
        "\n",
        "# Test\n",
        "data_np = np.random.randn(10000000)\n",
        "data_cp = cp.asarray(data_np)\n",
        "\n",
        "result_np = compute_stats_numpy(data_np)\n",
        "result_cp = compute_stats_cupy(data_cp).get()\n",
        "\n",
        "print(f\"NumPy result: {result_np}\")\n",
        "print(f\"CuPy result: {result_cp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b0bb4db",
      "metadata": {
        "id": "0b0bb4db"
      },
      "source": [
        "### Random Number Generation\n",
        "\n",
        "CuPy provides GPU-accelerated random number generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d3f052ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3f052ad",
        "outputId": "93956cb6-a883-46f2-f59f-59bbfd65627a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform mean: 0.4997\n",
            "Normal mean: -0.0002\n",
            "Integer mean: 49.5714\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# Set seed for reproducibility\n",
        "cp.random.seed(42)\n",
        "\n",
        "# Generate random numbers on GPU\n",
        "uniform = cp.random.random(1000000)\n",
        "normal = cp.random.randn(1000000)\n",
        "integers = cp.random.randint(0, 100, size=1000000)\n",
        "\n",
        "print(f\"Uniform mean: {cp.mean(uniform):.4f}\")\n",
        "print(f\"Normal mean: {cp.mean(normal):.4f}\")\n",
        "print(f\"Integer mean: {cp.mean(integers):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0752e71e",
      "metadata": {
        "id": "0752e71e"
      },
      "source": [
        "### Performance Comparison\n",
        "\n",
        "Let's compare NumPy and CuPy for matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "75948286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75948286",
        "outputId": "0ae5020f-f22b-46f7-e609-d7102475aa6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size 100x100:\n",
            "  NumPy: 0.04 ms\n",
            "  CuPy:  0.06 ms\n",
            "  Speedup: 0.6x\n",
            "\n",
            "Size 500x500:\n",
            "  NumPy: 0.70 ms\n",
            "  CuPy:  0.08 ms\n",
            "  Speedup: 8.2x\n",
            "\n",
            "Size 1000x1000:\n",
            "  NumPy: 3.79 ms\n",
            "  CuPy:  0.22 ms\n",
            "  Speedup: 17.2x\n",
            "\n",
            "Size 2000x2000:\n",
            "  NumPy: 26.96 ms\n",
            "  CuPy:  1.17 ms\n",
            "  Speedup: 23.1x\n",
            "\n",
            "Size 4000x4000:\n",
            "  NumPy: 205.83 ms\n",
            "  CuPy:  10.48 ms\n",
            "  Speedup: 19.6x\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "\n",
        "def benchmark_matmul(size, n_runs=10):\n",
        "    \"\"\"Compare NumPy vs CuPy matrix multiplication.\"\"\"\n",
        "    # Create random matrices\n",
        "    A_np = np.random.randn(size, size).astype(np.float32)\n",
        "    B_np = np.random.randn(size, size).astype(np.float32)\n",
        "\n",
        "    A_cp = cp.asarray(A_np)\n",
        "    B_cp = cp.asarray(B_np)\n",
        "\n",
        "    # Warm up GPU\n",
        "    _ = cp.dot(A_cp, B_cp)\n",
        "    cp.cuda.Stream.null.synchronize()\n",
        "\n",
        "    # Time NumPy\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(n_runs):\n",
        "        C_np = np.dot(A_np, B_np)\n",
        "    numpy_time = (time.perf_counter() - start) / n_runs\n",
        "\n",
        "    # Time CuPy\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(n_runs):\n",
        "        C_cp = cp.dot(A_cp, B_cp)\n",
        "        cp.cuda.Stream.null.synchronize()  # Wait for GPU to finish\n",
        "    cupy_time = (time.perf_counter() - start) / n_runs\n",
        "\n",
        "    print(f\"Size {size}x{size}:\")\n",
        "    print(f\"  NumPy: {numpy_time*1000:.2f} ms\")\n",
        "    print(f\"  CuPy:  {cupy_time*1000:.2f} ms\")\n",
        "    print(f\"  Speedup: {numpy_time/cupy_time:.1f}x\")\n",
        "\n",
        "\n",
        "# Test different sizes\n",
        "for size in [100, 500, 1000, 2000, 4000]:\n",
        "    benchmark_matmul(size)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4391f1e",
      "metadata": {
        "id": "f4391f1e"
      },
      "source": [
        "Typical results show:\n",
        "- Small matrices (100x100): GPU may be slower due to transfer overhead\n",
        "- Large matrices (2000x2000+): GPU can be 10-50x faster\n",
        "\n",
        "### Question\n",
        "\n",
        "Consider the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6318197d",
      "metadata": {
        "id": "6318197d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "data = np.random.randn(1000000)\n",
        "\n",
        "# Version A\n",
        "for i in range(100):\n",
        "    gpu_data = cp.asarray(data)\n",
        "    result = cp.sum(gpu_data ** 2)\n",
        "    cpu_result = result.get()\n",
        "\n",
        "# Version B\n",
        "gpu_data = cp.asarray(data)\n",
        "for i in range(100):\n",
        "    result = cp.sum(gpu_data ** 2)\n",
        "cpu_result = result.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe86b7b",
      "metadata": {
        "id": "efe86b7b"
      },
      "source": [
        "Which version is faster, and why?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Version B is significantly faster. In Version A, data is transferred to the GPU 100 times (once per iteration). In Version B, data is transferred once before the loop. Since data transfer between CPU and GPU is slow relative to computation, Version A wastes time on redundant transfers. Always minimize data movement between host and device."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e4e9585",
      "metadata": {
        "id": "6e4e9585"
      },
      "source": [
        "## PyTorch for GPU Computing\n",
        "\n",
        "PyTorch is primarily known as a deep learning framework, but its GPU tensor operations are useful for general scientific computing. PyTorch offers some advantages over CuPy:\n",
        "\n",
        "* Automatic differentiation (useful for optimization)\n",
        "* Broader ecosystem and community support\n",
        "* Easy model deployment\n",
        "\n",
        "### Tensors: PyTorch's Array Type\n",
        "\n",
        "PyTorch uses tensors instead of arrays. Tensors are similar to NumPy arrays but can live on GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "37eb46eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37eb46eb",
        "outputId": "c9e1ba66-f583-4171-bfed-4434cc2c2403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([1., 2., 3., 4., 5.])\n",
            "b shape: torch.Size([3, 4])\n",
            "c:\n",
            "tensor([[-0.2688, -0.7397, -0.2322],\n",
            "        [-0.5727, -0.5272, -1.8478]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create tensors\n",
        "a = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "b = torch.zeros(3, 4)  # 3x4 matrix of zeros\n",
        "c = torch.randn(2, 3)  # 2x3 matrix of random normal values\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"b shape: {b.shape}\")\n",
        "print(f\"c:\\n{c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ea1353",
      "metadata": {
        "id": "d4ea1353"
      },
      "source": [
        "### Moving Tensors to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "90adf5b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90adf5b1",
        "outputId": "61477019-7d75-489e-cf3d-a34fbab368cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "x_cpu device: cpu\n",
            "x_gpu device: cuda:0\n",
            "y_gpu device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create tensor on CPU\n",
        "x_cpu = torch.randn(1000, 1000)\n",
        "print(f\"x_cpu device: {x_cpu.device}\")\n",
        "\n",
        "# Move to GPU\n",
        "x_gpu = x_cpu.to(device)\n",
        "print(f\"x_gpu device: {x_gpu.device}\")\n",
        "\n",
        "# Alternative: create directly on GPU\n",
        "y_gpu = torch.randn(1000, 1000, device=device)\n",
        "print(f\"y_gpu device: {y_gpu.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d1bb5e",
      "metadata": {
        "id": "87d1bb5e"
      },
      "source": [
        "Common patterns for device management:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f3fdc5e0",
      "metadata": {
        "id": "f3fdc5e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Method 1: .to(device)\n",
        "x = torch.randn(100).to(device)\n",
        "\n",
        "# Method 2: .cuda() (only works if CUDA available)\n",
        "if torch.cuda.is_available():\n",
        "    y = torch.randn(100).cuda()\n",
        "\n",
        "# Method 3: Create directly on device\n",
        "z = torch.randn(100, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a447a784",
      "metadata": {
        "id": "a447a784"
      },
      "source": [
        "### Converting Between PyTorch and NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26fd89d",
      "metadata": {
        "id": "a26fd89d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# NumPy to PyTorch (CPU)\n",
        "np_array = np.array([1.0, 2.0, 3.0])\n",
        "torch_tensor = torch.from_numpy(np_array)\n",
        "\n",
        "# PyTorch (CPU) to NumPy\n",
        "back_to_numpy = torch_tensor.numpy()\n",
        "\n",
        "# For GPU tensors, must move to CPU first\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gpu_tensor = torch.randn(5, device=device)\n",
        "cpu_numpy = gpu_tensor.cpu().numpy()\n",
        "\n",
        "print(f\"Original NumPy: {np_array}\")\n",
        "print(f\"Torch tensor: {torch_tensor}\")\n",
        "print(f\"Back to NumPy: {back_to_numpy}\")\n",
        "print(f\"GPU tensor to NumPy: {cpu_numpy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d040f794",
      "metadata": {
        "id": "d040f794"
      },
      "source": [
        "### Basic Operations\n",
        "\n",
        "PyTorch operations mirror NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "643cb24c",
      "metadata": {
        "id": "643cb24c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create tensors on GPU\n",
        "a = torch.randn(1000, 1000, device=device)\n",
        "b = torch.randn(1000, 1000, device=device)\n",
        "\n",
        "# Element-wise operations\n",
        "c = a + b\n",
        "d = a * b\n",
        "e = torch.exp(a)\n",
        "\n",
        "# Matrix operations\n",
        "f = torch.mm(a, b)  # Matrix multiplication\n",
        "g = torch.inverse(a)  # Matrix inverse\n",
        "\n",
        "# Reductions\n",
        "h = torch.sum(a)\n",
        "i = torch.mean(a, dim=0)  # Mean along rows\n",
        "\n",
        "# Linear algebra\n",
        "u, s, v = torch.svd(a)\n",
        "\n",
        "print(f\"Matrix product shape: {f.shape}\")\n",
        "print(f\"Sum: {h.item():.4f}\")  # .item() extracts scalar value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d0bf2f",
      "metadata": {
        "id": "a5d0bf2f"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "A unique feature of PyTorch is automatic differentiation. This computes gradients automatically, which is useful for optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c7a8ffb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a8ffb4",
        "outputId": "a006bb49-b156-421d-b94b-82014436e9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([2., 3.], device='cuda:0', requires_grad=True)\n",
            "y = x[0]^2 + 3*x[1]^2 = 31.0\n",
            "dy/dx = tensor([ 4., 18.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create tensor with gradient tracking\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True, device='cuda')\n",
        "\n",
        "# Compute a function\n",
        "y = x[0]**2 + 3*x[1]**2\n",
        "\n",
        "# Compute gradient\n",
        "y.backward()\n",
        "\n",
        "# Access gradients\n",
        "print(f\"x = {x}\")\n",
        "print(f\"y = x[0]^2 + 3*x[1]^2 = {y.item()}\")\n",
        "print(f\"dy/dx = {x.grad}\")  # Should be [2*x[0], 6*x[1]] = [4, 18]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214b09e1",
      "metadata": {
        "id": "214b09e1"
      },
      "source": [
        "This is particularly useful for:\n",
        "* Maximum likelihood estimation\n",
        "* Gradient-based optimization\n",
        "* Neural network training\n",
        "\n",
        "### Question\n",
        "\n",
        "What happens if you try to perform an operation between a CPU tensor and a GPU tensor in PyTorch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "080b35d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "080b35d7",
        "outputId": "81a477eb-f285-44a3-e404-7935daa18342"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1779306464.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CPU tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# GPU tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m  \u001b[0;31m# What happens?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.randn(100)  # CPU tensor\n",
        "b = torch.randn(100, device='cuda')  # GPU tensor\n",
        "c = a + b  # What happens?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96f93c2",
      "metadata": {
        "id": "f96f93c2"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "PyTorch raises a RuntimeError because tensors must be on the same device for operations. The error message will say something like \"Expected all tensors to be on the same device.\" You must explicitly move tensors to the same device before operating on them: either `a.cuda() + b` or `a + b.cpu()`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8723e3c2",
      "metadata": {
        "id": "8723e3c2"
      },
      "source": [
        "## Practical Considerations\n",
        "\n",
        "### Timing GPU Operations\n",
        "\n",
        "GPU operations are asynchronous. When you call a GPU function, Python returns immediately while the GPU continues working. To get accurate timing, you must synchronize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5d29f31d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d29f31d",
        "outputId": "33e1e9bf-147d-4a23-c443-40f5e3834390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong measurement: 11.56 ms\n",
            "Correct measurement: 17.15 ms\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "a = torch.randn(5000, 5000, device=device)\n",
        "b = torch.randn(5000, 5000, device=device)\n",
        "\n",
        "# WRONG: This doesn't measure GPU time correctly\n",
        "start = time.perf_counter()\n",
        "c = torch.mm(a, b)\n",
        "wrong_time = time.perf_counter() - start\n",
        "print(f\"Wrong measurement: {wrong_time*1000:.2f} ms\")\n",
        "\n",
        "# CORRECT: Synchronize before measuring\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "c = torch.mm(a, b)\n",
        "torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "correct_time = time.perf_counter() - start\n",
        "print(f\"Correct measurement: {correct_time*1000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2b4aa0",
      "metadata": {
        "id": "7a2b4aa0"
      },
      "source": [
        "For CuPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ccea46",
      "metadata": {
        "id": "08ccea46"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "a = cp.random.randn(5000, 5000)\n",
        "b = cp.random.randn(5000, 5000)\n",
        "\n",
        "# Synchronize with CuPy\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "start = time.perf_counter()\n",
        "c = cp.dot(a, b)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "elapsed = time.perf_counter() - start\n",
        "print(f\"Time: {elapsed*1000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35de46d2",
      "metadata": {
        "id": "35de46d2"
      },
      "source": [
        "### Memory Management\n",
        "\n",
        "GPU memory is limited (typically 8-24 GB on consumer GPUs). Monitor and manage it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "35edf90e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35edf90e",
        "outputId": "b5a2ea6a-6f28-44fd-fbe4-0c2735295247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated: 0.32 GB\n",
            "Cached: 0.43 GB\n",
            "After allocation: 0.72 GB\n",
            "After cleanup: 0.32 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Check memory usage\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "    # Create a large tensor\n",
        "    x = torch.randn(10000, 10000, device='cuda')\n",
        "    print(f\"After allocation: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    # Delete and clear cache\n",
        "    del x\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"After cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc7dce5",
      "metadata": {
        "id": "5fc7dce5"
      },
      "source": [
        "### Common Pitfalls\n",
        "\n",
        "**1. Data transfer overhead**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02eb950",
      "metadata": {
        "id": "e02eb950"
      },
      "outputs": [],
      "source": [
        "# BAD: Transferring inside a loop\n",
        "for i in range(1000):\n",
        "    x_gpu = torch.tensor(data[i]).cuda()\n",
        "    result = process(x_gpu)\n",
        "    results.append(result.cpu())\n",
        "\n",
        "# GOOD: Batch transfer\n",
        "x_gpu = torch.tensor(data).cuda()\n",
        "results_gpu = process_batch(x_gpu)\n",
        "results = results_gpu.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b8d2a4",
      "metadata": {
        "id": "30b8d2a4"
      },
      "source": [
        "**2. Forgetting to synchronize for timing**\n",
        "\n",
        "Always call `torch.cuda.synchronize()` or `cp.cuda.Stream.null.synchronize()` before timing measurements.\n",
        "\n",
        "**3. Operations between different devices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdeff7c8",
      "metadata": {
        "id": "cdeff7c8"
      },
      "outputs": [],
      "source": [
        "# This will error\n",
        "a_cpu = torch.randn(100)\n",
        "b_gpu = torch.randn(100, device='cuda')\n",
        "# c = a_cpu + b_gpu  # RuntimeError!\n",
        "\n",
        "# Fix: move to same device\n",
        "c = a_cpu.cuda() + b_gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afb3294",
      "metadata": {
        "id": "7afb3294"
      },
      "source": [
        "**4. Using GPU for small operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4fbaadb",
      "metadata": {
        "id": "e4fbaadb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "# Small operation: CPU is faster due to transfer overhead\n",
        "small = torch.randn(100)\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "result = small.to(device).sum().cpu()\n",
        "torch.cuda.synchronize()\n",
        "print(f\"Small tensor GPU: {(time.perf_counter()-start)*1000:.3f} ms\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "result = small.sum()\n",
        "print(f\"Small tensor CPU: {(time.perf_counter()-start)*1000:.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee419aec",
      "metadata": {
        "id": "ee419aec"
      },
      "source": [
        "### Question\n",
        "\n",
        "A researcher has the following code that runs slowly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8a66bd",
      "metadata": {
        "id": "ea8a66bd"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for i in range(10000):\n",
        "    x = cp.asarray(data[i])  # data[i] is a NumPy array of shape (100,)\n",
        "    y = cp.sum(x ** 2)\n",
        "    results.append(y.get())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50eff899",
      "metadata": {
        "id": "50eff899"
      },
      "source": [
        "What is the main performance problem, and how would you fix it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The problem is that each of the 10,000 iterations transfers a small array (100 elements) to the GPU and transfers the result back. The data transfer overhead dominates the computation time.\n",
        "\n",
        "Fix by batching the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009ffbbe",
      "metadata": {
        "id": "009ffbbe"
      },
      "outputs": [],
      "source": [
        "# Stack all data into one array and transfer once\n",
        "all_data = cp.asarray(np.stack(data))  # Shape: (10000, 100)\n",
        "# Compute all results at once on GPU\n",
        "results_gpu = cp.sum(all_data ** 2, axis=1)  # Shape: (10000,)\n",
        "# Transfer back once\n",
        "results = results_gpu.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93863129",
      "metadata": {
        "id": "93863129"
      },
      "source": [
        "This reduces 20,000 transfers to just 2 (one in, one out) and uses GPU parallelism effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03b4210",
      "metadata": {
        "id": "d03b4210"
      },
      "source": [
        "## Statistical Computing Examples\n",
        "\n",
        "### Monte Carlo Pi Estimation\n",
        "\n",
        "A classic example to demonstrate GPU parallelism:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9b81953e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b81953e",
        "outputId": "ce5859c1-5852-42d6-99d3-6ece8f07edb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking with N=100,000,000 samples...\n",
            "Warming up GPU... (to exclude compilation time)\n",
            "--- Start Timing ---\n",
            "NumPy (float64):  pi = 3.141690, time = 2.476s\n",
            "CuPy (float32):   pi = 3.141643, time = 0.025s, speedup = 99.8x\n",
            "PyTorch (float32): pi = 3.141643, time = 0.024s, speedup = 101.2x\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "def estimate_pi_numpy(n):\n",
        "    \"\"\"Estimate pi using Monte Carlo - NumPy.\"\"\"\n",
        "    # NumPy defaults to float64\n",
        "    x = np.random.random(n).astype(np.float32)\n",
        "    y = np.random.random(n).astype(np.float32)\n",
        "    inside = np.sum(x**2 + y**2 <= 1)\n",
        "    return 4 * inside / n\n",
        "\n",
        "\n",
        "def estimate_pi_cupy(n):\n",
        "    \"\"\"Estimate pi using Monte Carlo - CuPy.\"\"\"\n",
        "    # Explicitly use float32 for GPU speed (GPUs are much faster at float32)\n",
        "    x = cp.random.random(n, dtype=cp.float32)\n",
        "    y = cp.random.random(n, dtype=cp.float32)\n",
        "    inside = cp.sum(x**2 + y**2 <= 1)\n",
        "    return 4 * float(inside.get()) / n\n",
        "\n",
        "\n",
        "def estimate_pi_torch(n, device):\n",
        "    \"\"\"Estimate pi using Monte Carlo - PyTorch.\"\"\"\n",
        "    # PyTorch defaults to float32\n",
        "    x = torch.rand(n, device=device)\n",
        "    y = torch.rand(n, device=device)\n",
        "    inside = torch.sum(x**2 + y**2 <= 1)\n",
        "    return 4 * inside.item() / n\n",
        "\n",
        "\n",
        "# Benchmark\n",
        "n = 100_000_000\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Benchmarking with N={n:,} samples...\")\n",
        "print(\"Warming up GPU... (to exclude compilation time)\")\n",
        "# Warmup\n",
        "estimate_pi_cupy(1000)\n",
        "if torch.cuda.is_available():\n",
        "    estimate_pi_torch(1000, device)\n",
        "\n",
        "print(\"--- Start Timing ---\")\n",
        "\n",
        "# NumPy\n",
        "start = time.perf_counter()\n",
        "pi_numpy = estimate_pi_numpy(n)\n",
        "numpy_time = time.perf_counter() - start\n",
        "\n",
        "# CuPy\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "start = time.perf_counter()\n",
        "pi_cupy = estimate_pi_cupy(n)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "cupy_time = time.perf_counter() - start\n",
        "\n",
        "# PyTorch\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "pi_torch = estimate_pi_torch(n, device)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = time.perf_counter() - start\n",
        "\n",
        "print(f\"NumPy (float64):  pi = {pi_numpy:.6f}, time = {numpy_time:.3f}s\")\n",
        "print(f\"CuPy (float32):   pi = {pi_cupy:.6f}, time = {cupy_time:.3f}s, speedup = {numpy_time/cupy_time:.1f}x\")\n",
        "print(f\"PyTorch (float32): pi = {pi_torch:.6f}, time = {torch_time:.3f}s, speedup = {numpy_time/torch_time:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f50c96",
      "metadata": {
        "id": "15f50c96"
      },
      "source": [
        "### Bootstrap Confidence Intervals\n",
        "\n",
        "Bootstrap resampling benefits from GPU parallelism:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ad970aa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad970aa7",
        "outputId": "49448aaf-3dfe-4612-a063-ff6a49afdb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap with 10000 resamples:\n",
            "NumPy:  CI = [-0.0221, 0.0176], time = 1.975s\n",
            "CuPy:   CI = [-0.0217, 0.0177], time = 0.504s\n",
            "Speedup: 3.9x\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "\n",
        "def bootstrap_mean_numpy(data, n_bootstrap=10000):\n",
        "    \"\"\"Bootstrap mean estimation - NumPy.\"\"\"\n",
        "    n = len(data)\n",
        "    means = np.empty(n_bootstrap)\n",
        "    for i in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means[i] = np.mean(sample)\n",
        "    return means\n",
        "\n",
        "\n",
        "def bootstrap_mean_cupy(data, n_bootstrap=10000):\n",
        "    \"\"\"Bootstrap mean estimation - CuPy (vectorized).\"\"\"\n",
        "    n = len(data)\n",
        "    # Generate all bootstrap indices at once\n",
        "    indices = cp.random.randint(0, n, size=(n_bootstrap, n))\n",
        "    # Gather samples and compute means\n",
        "    samples = data[indices]\n",
        "    means = cp.mean(samples, axis=1)\n",
        "    return means\n",
        "\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(42)\n",
        "data_np = np.random.randn(10000)\n",
        "data_cp = cp.asarray(data_np)\n",
        "\n",
        "n_bootstrap = 10000\n",
        "\n",
        "# NumPy\n",
        "start = time.perf_counter()\n",
        "means_np = bootstrap_mean_numpy(data_np, n_bootstrap)\n",
        "numpy_time = time.perf_counter() - start\n",
        "\n",
        "# CuPy\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "start = time.perf_counter()\n",
        "means_cp = bootstrap_mean_cupy(data_cp, n_bootstrap)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "cupy_time = time.perf_counter() - start\n",
        "\n",
        "# Compute confidence intervals\n",
        "ci_np = np.percentile(means_np, [2.5, 97.5])\n",
        "ci_cp = cp.percentile(means_cp, [2.5, 97.5]).get()\n",
        "\n",
        "print(f\"Bootstrap with {n_bootstrap} resamples:\")\n",
        "print(f\"NumPy:  CI = [{ci_np[0]:.4f}, {ci_np[1]:.4f}], time = {numpy_time:.3f}s\")\n",
        "print(f\"CuPy:   CI = [{ci_cp[0]:.4f}, {ci_cp[1]:.4f}], time = {cupy_time:.3f}s\")\n",
        "print(f\"Speedup: {numpy_time/cupy_time:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9bd2985",
      "metadata": {
        "id": "a9bd2985"
      },
      "source": [
        "### Linear Regression with GPU\n",
        "\n",
        "Matrix operations for linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "793fa711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793fa711",
        "outputId": "5caaf931-6617-4ba0-cf3b-edeedc9da0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS with n=100000, p=500\n",
            "NumPy:   time = 3.798s\n",
            "PyTorch: time = 0.242s\n",
            "Speedup: 15.7x\n",
            "Max coefficient difference: 0.000001\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "def ols_numpy(X, y):\n",
        "    \"\"\"Ordinary least squares - NumPy.\"\"\"\n",
        "    return np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "\n",
        "\n",
        "def ols_torch(X, y):\n",
        "    \"\"\"Ordinary least squares - PyTorch.\"\"\"\n",
        "    return torch.linalg.lstsq(X, y).solution\n",
        "\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(42)\n",
        "n, p = 100000, 500\n",
        "X_np = np.random.randn(n, p).astype(np.float32)\n",
        "true_beta = np.random.randn(p).astype(np.float32)\n",
        "y_np = X_np @ true_beta + 0.1 * np.random.randn(n).astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_torch = torch.from_numpy(X_np).to(device)\n",
        "y_torch = torch.from_numpy(y_np).to(device)\n",
        "\n",
        "# Benchmark\n",
        "start = time.perf_counter()\n",
        "beta_np = ols_numpy(X_np, y_np)\n",
        "numpy_time = time.perf_counter() - start\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "beta_torch = ols_torch(X_torch, y_torch)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = time.perf_counter() - start\n",
        "\n",
        "print(f\"OLS with n={n}, p={p}\")\n",
        "print(f\"NumPy:   time = {numpy_time:.3f}s\")\n",
        "print(f\"PyTorch: time = {torch_time:.3f}s\")\n",
        "print(f\"Speedup: {numpy_time/torch_time:.1f}x\")\n",
        "\n",
        "# Verify results are similar\n",
        "beta_torch_np = beta_torch.cpu().numpy()\n",
        "print(f\"Max coefficient difference: {np.max(np.abs(beta_np - beta_torch_np)):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0bf1489",
      "metadata": {
        "id": "e0bf1489"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* **GPU computing** accelerates data-parallel computations by using thousands of simple cores working simultaneously.\n",
        "\n",
        "* **CuPy** provides a NumPy-compatible interface for GPU arrays. Replace `numpy` with `cupy` and your code runs on GPU with minimal changes.\n",
        "\n",
        "* **PyTorch** offers GPU tensors plus automatic differentiation. Use it when you need gradients or are working near deep learning applications.\n",
        "\n",
        "* **Data transfer** between CPU and GPU has significant overhead. Minimize transfers by batching operations and keeping data on GPU.\n",
        "\n",
        "* **Timing GPU operations** requires synchronization. Always call `synchronize()` before timing measurements.\n",
        "\n",
        "* **GPU excels at** large matrix operations, element-wise computations on big arrays, and embarrassingly parallel tasks like Monte Carlo simulations.\n",
        "\n",
        "* **GPU is not suitable for** small data, sequential algorithms, or tasks with complex branching.\n",
        "\n",
        "## Recommended Resources\n",
        "\n",
        "* [CuPy Documentation](https://docs.cupy.dev/) - Official CuPy documentation\n",
        "* [CuPy User Guide](https://docs.cupy.dev/en/stable/user_guide/index.html) - Getting started with CuPy\n",
        "* [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - Official PyTorch documentation\n",
        "* [PyTorch CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html) - GPU programming with PyTorch\n",
        "* [NVIDIA CUDA Python](https://developer.nvidia.com/cuda-python) - NVIDIA's CUDA Python resources"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}