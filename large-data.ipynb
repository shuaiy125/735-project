{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "282f4f7d-816c-44f7-8285-65b1aa9e1213",
      "metadata": {
        "id": "282f4f7d-816c-44f7-8285-65b1aa9e1213"
      },
      "source": [
        "# Handling Large Data\n",
        "\n",
        "## Why Large Data Matters\n",
        "\n",
        "In biomedical research, we routinely encounter datasets that challenge the limits of standard computing resources. Genomic studies may involve millions of genetic variants across thousands of samples. Single-cell RNA sequencing can produce expression measurements for 30,000 genes across hundreds of thousands of cells. Electronic health records for large hospital systems contain millions of patient encounters with dozens of variables each.\n",
        "\n",
        "When your data doesn't fit comfortably in memory, naive approaches fail:\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# This might crash your computer if the file is large enough\n",
        "df = pd.read_csv(\"massive_patient_records.csv\")  # 50 GB file\n",
        "```\n",
        "\n",
        "The error message you'll see:\n",
        "\n",
        "```\n",
        "MemoryError: Unable to allocate 47.5 GiB for an array with shape (1000000000,) and data type float64\n",
        "```\n",
        "\n",
        "This lecture covers practical strategies for working with large datasets in Python: optimizing memory usage, processing data in chunks, using efficient file formats, and leveraging specialized data structures.\n",
        "\n",
        "Before starting, let's first create the example data files that will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v55t9yjcevf",
      "metadata": {
        "id": "v55t9yjcevf",
        "outputId": "e43f5322-c362-444d-997a-1690a48d52b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (1.15.3)\n",
            "Requirement already satisfied: h5py in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (3.15.1)\n",
            "Requirement already satisfied: pyarrow in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (23.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/daiwei/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install pandas numpy scipy h5py pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "krcwbwa6sed",
      "metadata": {
        "id": "krcwbwa6sed",
        "outputId": "dae5e229-8f98-4d26-e52c-59403562f112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created example files in data/:\n",
            "  patient_records.csv: 4282.5 KB\n",
            "  large_patient_file.csv: 4282.5 KB\n",
            "  raw_patients.csv: 4282.5 KB\n",
            "  expression_matrix.csv: 389.0 KB\n",
            "  expression_matrix.parquet: 334.2 KB\n"
          ]
        }
      ],
      "source": [
        "# Setup: Create example data files used throughout this notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create patient_records.csv (used in pandas optimization examples)\n",
        "n_patients = 100000\n",
        "patients_df = pd.DataFrame({\n",
        "    \"patient_id\": range(n_patients),\n",
        "    \"age\": np.random.randint(18, 90, n_patients),\n",
        "    \"systolic_bp\": np.random.normal(120, 15, n_patients),\n",
        "    \"diagnosis\": np.random.choice([\"healthy\", \"diabetes\", \"hypertension\"], n_patients),\n",
        "    \"hospital\": np.random.choice([\"UNC\", \"Duke\", \"Wake Forest\", \"ECU\"], n_patients)\n",
        "})\n",
        "patients_df.to_csv(\"data/patient_records.csv\", index=False)\n",
        "\n",
        "# Create large_patient_file.csv (used in chunking examples)\n",
        "patients_df.to_csv(\"data/large_patient_file.csv\", index=False)\n",
        "\n",
        "# Create raw_patients.csv (used in transform_and_save example)\n",
        "patients_df.to_csv(\"data/raw_patients.csv\", index=False)\n",
        "\n",
        "# Create expression_matrix.csv (gene expression data)\n",
        "n_genes = 1000  # Small for demo; real data would have ~20,000 genes\n",
        "n_samples = 50  # Small for demo; real data would have hundreds\n",
        "gene_symbols = [f\"GENE_{i}\" for i in range(n_genes)]\n",
        "sample_cols = [f\"sample_{i:03d}\" for i in range(n_samples)]\n",
        "\n",
        "expression_data = {\"gene_symbol\": gene_symbols}\n",
        "for col in sample_cols:\n",
        "    expression_data[col] = np.random.randint(0, 50000, n_genes).astype(float)\n",
        "expression_df = pd.DataFrame(expression_data)\n",
        "expression_df.to_csv(\"data/expression_matrix.csv\", index=False)\n",
        "\n",
        "# Also save as parquet for parquet examples\n",
        "expression_df.to_parquet(\"data/expression_matrix.parquet\")\n",
        "\n",
        "print(\"Created example files in data/:\")\n",
        "for f in [\"patient_records.csv\", \"large_patient_file.csv\", \"raw_patients.csv\",\n",
        "          \"expression_matrix.csv\", \"expression_matrix.parquet\"]:\n",
        "    size_kb = os.path.getsize(f\"data/{f}\") / 1024\n",
        "    print(f\"  {f}: {size_kb:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa129fb7-a346-423c-a6b2-df8c8b09e189",
      "metadata": {
        "id": "aa129fb7-a346-423c-a6b2-df8c8b09e189"
      },
      "source": [
        "### Understanding Memory Usage\n",
        "\n",
        "Before optimizing, we need to measure. Python provides several tools for understanding memory consumption.\n",
        "\n",
        "The `sys.getsizeof()` function returns the size of an object in bytes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfc1ebd",
      "metadata": {
        "id": "edfc1ebd",
        "outputId": "d4878b24-6d47-471e-fd9f-108404df91dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Integer: 28 bytes\n",
            "Float: 24 bytes\n",
            "String: 46 bytes\n",
            "List of 5 ints: 104 bytes\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "x = 42\n",
        "print(f\"Integer: {sys.getsizeof(x)} bytes\")  # 28 bytes\n",
        "\n",
        "y = 3.14159\n",
        "print(f\"Float: {sys.getsizeof(y)} bytes\")  # 24 bytes\n",
        "\n",
        "s = \"hello\"\n",
        "print(f\"String: {sys.getsizeof(s)} bytes\")  # 54 bytes\n",
        "\n",
        "# A list has overhead beyond its elements\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "print(f\"List of 5 ints: {sys.getsizeof(numbers)} bytes\")  # 104 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23dad819",
      "metadata": {
        "id": "23dad819"
      },
      "source": [
        "For pandas DataFrames, use the `memory_usage()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a559640d",
      "metadata": {
        "id": "a559640d",
        "outputId": "ca807b91-cf27-4299-d9f5-e2334d8d24bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index              132\n",
            "patient_id      800000\n",
            "age             800000\n",
            "systolic_bp     800000\n",
            "diagnosis      5800691\n",
            "hospital       5425173\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample clinical dataset\n",
        "n_patients = 100000\n",
        "df = pd.DataFrame({\n",
        "    \"patient_id\": range(n_patients),\n",
        "    \"age\": np.random.randint(18, 90, n_patients),\n",
        "    \"systolic_bp\": np.random.normal(120, 15, n_patients),\n",
        "    \"diagnosis\": np.random.choice([\"healthy\", \"diabetes\", \"hypertension\"], n_patients),\n",
        "    \"hospital\": np.random.choice([\"UNC\", \"Duke\", \"Wake Forest\", \"ECU\"], n_patients)\n",
        "})\n",
        "\n",
        "# Check memory usage by column\n",
        "print(df.memory_usage(deep=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5058a1",
      "metadata": {
        "id": "9c5058a1"
      },
      "source": [
        "Output:\n",
        "```\n",
        "Index           128\n",
        "patient_id    800000\n",
        "age           800000\n",
        "systolic_bp   800000\n",
        "diagnosis    6400000\n",
        "hospital     6400000\n",
        "dtype: int64\n",
        "```\n",
        "\n",
        "The `deep=True` parameter is important for accurate measurement of string columns. Notice how the string columns (`diagnosis` and `hospital`) use 8x more memory than the numeric columns, even though they have only a few unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9146001c",
      "metadata": {
        "id": "9146001c",
        "outputId": "2b9e31e8-0402-4ec9-c7a9-4cc587e4cfdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total memory: 12.99 MB\n"
          ]
        }
      ],
      "source": [
        "# Total memory in megabytes\n",
        "total_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "print(f\"Total memory: {total_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54dd430",
      "metadata": {
        "id": "f54dd430"
      },
      "source": [
        "### The Memory Problem with Default Data Types\n",
        "\n",
        "Pandas uses memory-inefficient defaults. Let's examine why:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dab32cd",
      "metadata": {
        "id": "1dab32cd",
        "outputId": "a90f70dc-fdb0-405e-fb2c-d49ff0707e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "patient_id       int64\n",
            "age              int64\n",
            "systolic_bp    float64\n",
            "diagnosis       object\n",
            "hospital        object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2e799b",
      "metadata": {
        "id": "cc2e799b"
      },
      "source": [
        "Output:\n",
        "```\n",
        "patient_id       int64\n",
        "age              int64\n",
        "systolic_bp    float64\n",
        "diagnosis       object\n",
        "hospital        object\n",
        "dtype: object\n",
        "```\n",
        "\n",
        "The `int64` type uses 8 bytes per value, even for columns like `age` that only need values 0-120. The `object` type for strings stores each string as a separate Python object with significant overhead."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d7e3ff",
      "metadata": {
        "id": "71d7e3ff"
      },
      "source": [
        "## Pandas Memory Optimization\n",
        "\n",
        "The most impactful optimization is choosing appropriate data types. This section covers techniques that can reduce memory usage by 50-90%.\n",
        "\n",
        "### Numeric Type Downcasting\n",
        "\n",
        "For integer columns, choose the smallest type that fits your data:\n",
        "\n",
        "| Type    | Bytes | Range |\n",
        "|---------|-------|-------|\n",
        "| int8    | 1     | -128 to 127 |\n",
        "| int16   | 2     | -32,768 to 32,767 |\n",
        "| int32   | 4     | -2.1 billion to 2.1 billion |\n",
        "| int64   | 8     | -9.2 quintillion to 9.2 quintillion |\n",
        "| uint8   | 1     | 0 to 255 |\n",
        "| uint16  | 2     | 0 to 65,535 |\n",
        "\n",
        "For floating-point columns:\n",
        "\n",
        "| Type    | Bytes | Precision |\n",
        "|---------|-------|-----------|\n",
        "| float16 | 2     | ~3 decimal digits |\n",
        "| float32 | 4     | ~7 decimal digits |\n",
        "| float64 | 8     | ~15 decimal digits |\n",
        "\n",
        "Let's optimize our clinical dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78830760",
      "metadata": {
        "id": "78830760",
        "outputId": "f1e6be21-1f19-4902-e700-2bf20ae582de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: 12.99 MB\n",
            "patient_id      uint32\n",
            "age              uint8\n",
            "systolic_bp    float32\n",
            "diagnosis       object\n",
            "hospital        object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Before optimization\n",
        "print(f\"Before: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Downcast integers\n",
        "df[\"patient_id\"] = pd.to_numeric(df[\"patient_id\"], downcast=\"unsigned\")\n",
        "df[\"age\"] = pd.to_numeric(df[\"age\"], downcast=\"unsigned\")\n",
        "\n",
        "# Downcast floats (be careful with precision requirements)\n",
        "df[\"systolic_bp\"] = pd.to_numeric(df[\"systolic_bp\"], downcast=\"float\")\n",
        "\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50619764",
      "metadata": {
        "id": "50619764"
      },
      "source": [
        "Output:\n",
        "```\n",
        "patient_id      uint32\n",
        "age              uint8\n",
        "systolic_bp    float32\n",
        "diagnosis       object\n",
        "hospital        object\n",
        "dtype: object\n",
        "```\n",
        "\n",
        "The `age` column now uses 1 byte instead of 8, and `systolic_bp` uses 4 bytes instead of 8.\n",
        "\n",
        "### The Categorical Data Type\n",
        "\n",
        "For string columns with few unique values, the `category` dtype provides dramatic savings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314ed3fe",
      "metadata": {
        "id": "314ed3fe",
        "outputId": "be0bf508-4137-4e15-d93e-a4c06e2a13ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique diagnoses: 3\n",
            "Unique hospitals: 4\n",
            "After: 1.05 MB\n"
          ]
        }
      ],
      "source": [
        "# Check unique values\n",
        "print(f\"Unique diagnoses: {df['diagnosis'].nunique()}\")  # 3\n",
        "print(f\"Unique hospitals: {df['hospital'].nunique()}\")   # 4\n",
        "\n",
        "# Convert to categorical\n",
        "df[\"diagnosis\"] = df[\"diagnosis\"].astype(\"category\")\n",
        "df[\"hospital\"] = df[\"hospital\"].astype(\"category\")\n",
        "\n",
        "print(f\"After: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa1c617",
      "metadata": {
        "id": "6fa1c617"
      },
      "source": [
        "The categorical type stores an integer code for each row plus a small lookup table of unique values. For 100,000 rows with only 3-4 unique values, this reduces memory from ~6 MB per column to ~100 KB.\n",
        "\n",
        "### Specifying Types at Load Time\n",
        "\n",
        "Rather than loading data and then converting, specify types when reading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ba0039",
      "metadata": {
        "id": "33ba0039"
      },
      "outputs": [],
      "source": [
        "# Define the optimal types\n",
        "dtype_spec = {\n",
        "    \"patient_id\": \"uint32\",\n",
        "    \"age\": \"uint8\",\n",
        "    \"systolic_bp\": \"float32\",\n",
        "    \"diagnosis\": \"category\",\n",
        "    \"hospital\": \"category\"\n",
        "}\n",
        "\n",
        "# Load with specified types\n",
        "df = pd.read_csv(\"data/patient_records.csv\", dtype=dtype_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0174aa3e",
      "metadata": {
        "id": "0174aa3e"
      },
      "source": [
        "This avoids the memory spike from loading with default types first.\n",
        "\n",
        "### Loading Only Needed Columns\n",
        "\n",
        "If you only need a subset of columns, use `usecols`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94552d9",
      "metadata": {
        "id": "c94552d9"
      },
      "outputs": [],
      "source": [
        "# Only load the columns we need for analysis\n",
        "df = pd.read_csv(\n",
        "    \"data/patient_records.csv\",\n",
        "    usecols=[\"patient_id\", \"age\", \"diagnosis\"],\n",
        "    dtype={\"patient_id\": \"uint32\", \"age\": \"uint8\", \"diagnosis\": \"category\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39e1fb4",
      "metadata": {
        "id": "e39e1fb4"
      },
      "source": [
        "For files with many columns, this can dramatically reduce memory usage and load time.\n",
        "\n",
        "### Question\n",
        "\n",
        "Consider this code that loads a gene expression dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b105290",
      "metadata": {
        "id": "9b105290",
        "outputId": "a9bb8806-c2bf-4b6a-df3c-e4effa89226c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gene_symbol     object\n",
            "sample_000     float64\n",
            "sample_001     float64\n",
            "sample_002     float64\n",
            "sample_003     float64\n",
            "sample_004     float64\n",
            "sample_005     float64\n",
            "sample_006     float64\n",
            "sample_007     float64\n",
            "sample_008     float64\n",
            "sample_009     float64\n",
            "sample_010     float64\n",
            "sample_011     float64\n",
            "sample_012     float64\n",
            "sample_013     float64\n",
            "sample_014     float64\n",
            "sample_015     float64\n",
            "sample_016     float64\n",
            "sample_017     float64\n",
            "sample_018     float64\n",
            "sample_019     float64\n",
            "sample_020     float64\n",
            "sample_021     float64\n",
            "sample_022     float64\n",
            "sample_023     float64\n",
            "sample_024     float64\n",
            "sample_025     float64\n",
            "sample_026     float64\n",
            "sample_027     float64\n",
            "sample_028     float64\n",
            "sample_029     float64\n",
            "sample_030     float64\n",
            "sample_031     float64\n",
            "sample_032     float64\n",
            "sample_033     float64\n",
            "sample_034     float64\n",
            "sample_035     float64\n",
            "sample_036     float64\n",
            "sample_037     float64\n",
            "sample_038     float64\n",
            "sample_039     float64\n",
            "sample_040     float64\n",
            "sample_041     float64\n",
            "sample_042     float64\n",
            "sample_043     float64\n",
            "sample_044     float64\n",
            "sample_045     float64\n",
            "sample_046     float64\n",
            "sample_047     float64\n",
            "sample_048     float64\n",
            "sample_049     float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Gene expression matrix: 20,000 genes x 500 samples\n",
        "# Expression values range from 0 to ~50,000 (RNA-seq counts)\n",
        "# Sample IDs are strings like \"TCGA-AB-1234\"\n",
        "# Gene symbols are strings like \"TP53\", \"BRCA1\", etc.\n",
        "\n",
        "df = pd.read_csv(\"data/expression_matrix.csv\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d267109",
      "metadata": {
        "id": "7d267109"
      },
      "source": [
        "Output:\n",
        "```\n",
        "gene_symbol      object\n",
        "sample_001      float64\n",
        "sample_002      float64\n",
        "...\n",
        "sample_500      float64\n",
        "```\n",
        "\n",
        "The DataFrame uses 80 MB of memory. Describe three specific optimizations that could reduce memory usage and estimate the savings for each.\n",
        "\n",
        "#### Answer\n",
        "\n",
        "1. **Convert `gene_symbol` to categorical**: With ~20,000 unique gene symbols, categorical won't save much here since most values are unique. Skip this optimization for this column.\n",
        "\n",
        "2. **Downcast expression values to float32**: RNA-seq counts don't need 15 decimal places of precision. Converting from float64 to float32 cuts memory in half for the numeric columns. Since expression values dominate the memory usage (500 columns x 20,000 rows x 8 bytes = 80 MB), this saves ~40 MB.\n",
        "\n",
        "3. **Consider uint16 if counts are integers**: If the values are truly counts (integers 0-50,000), `uint16` (range 0-65,535) would use only 2 bytes per value instead of 8, saving 75% of numeric memory (~60 MB savings).\n",
        "\n",
        "4. **Use `usecols` if analyzing a subset**: If only analyzing a subset of samples, loading only those columns avoids loading unnecessary data entirely.\n",
        "\n",
        "Optimized loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432a3879",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "432a3879"
      },
      "outputs": [],
      "source": [
        "# If counts are integers 0-65535\n",
        "df = pd.read_csv(\n",
        "    \"data/expression_matrix.csv\",\n",
        "    dtype={col: \"uint16\" for col in pd.read_csv(\"data/expression_matrix.csv\", nrows=0).columns[1:]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11f5d6b",
      "metadata": {
        "id": "d11f5d6b"
      },
      "source": [
        "## Processing Data in Chunks\n",
        "\n",
        "When data is too large to fit in memory even with optimizations, process it in chunks. This approach trades memory for processing time.\n",
        "\n",
        "### The chunksize Parameter\n",
        "\n",
        "Many pandas readers support the `chunksize` parameter, which returns an iterator instead of a DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2686329e",
      "metadata": {
        "id": "2686329e",
        "outputId": "ff71338c-6da0-4cb6-9f1e-8df6822759ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n",
            "Processing chunk with 10000 rows\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Process a large file 10,000 rows at a time\n",
        "chunk_iter = pd.read_csv(\"data/large_patient_file.csv\", chunksize=10000)\n",
        "\n",
        "# Each iteration yields a DataFrame with 10,000 rows\n",
        "for chunk in chunk_iter:\n",
        "    print(f\"Processing chunk with {len(chunk)} rows\")\n",
        "    # Process this chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "931a5e31",
      "metadata": {
        "id": "931a5e31"
      },
      "source": [
        "### Aggregating Across Chunks\n",
        "\n",
        "A common pattern is computing statistics across the entire dataset by combining partial results from each chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb58cb11",
      "metadata": {
        "id": "fb58cb11",
        "outputId": "223175fd-00ff-46d9-a5db-3eddfc8ccf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'diabetes': 53.32473124755337, 'healthy': 53.41256397210667, 'hypertension': 53.30394271675954}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_mean_age_by_diagnosis(filepath, chunksize=50000):\n",
        "    \"\"\"Compute mean age for each diagnosis across a large file.\"\"\"\n",
        "    # Accumulators\n",
        "    sums = {}\n",
        "    counts = {}\n",
        "\n",
        "    for chunk in pd.read_csv(filepath, chunksize=chunksize):\n",
        "        # Group by diagnosis within this chunk\n",
        "        grouped = chunk.groupby(\"diagnosis\")[\"age\"]\n",
        "\n",
        "        for diagnosis, group in grouped:\n",
        "            if diagnosis not in sums:\n",
        "                sums[diagnosis] = 0\n",
        "                counts[diagnosis] = 0\n",
        "            sums[diagnosis] += group.sum()\n",
        "            counts[diagnosis] += len(group)\n",
        "\n",
        "    # Compute final means\n",
        "    return {diag: sums[diag] / counts[diag] for diag in sums}\n",
        "\n",
        "result = compute_mean_age_by_diagnosis(\"data/large_patient_file.csv\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fc897a",
      "metadata": {
        "id": "59fc897a"
      },
      "source": [
        "### Chunked Processing with Generators\n",
        "\n",
        "Generators provide a clean way to build streaming data pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db70eb59",
      "metadata": {
        "id": "db70eb59",
        "outputId": "784b9bc6-7d07-47c0-f3f9-ea9e37242a13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean systolic BP for patients over 65: 120.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_and_filter(filepath, chunksize=50000):\n",
        "    \"\"\"Yield filtered chunks from a large CSV.\"\"\"\n",
        "    for chunk in pd.read_csv(filepath, chunksize=chunksize):\n",
        "        # Filter to patients over 65\n",
        "        filtered = chunk[chunk[\"age\"] > 65]\n",
        "        if len(filtered) > 0:\n",
        "            yield filtered\n",
        "\n",
        "def process_elderly_patients(filepath):\n",
        "    \"\"\"Process elderly patients without loading entire file.\"\"\"\n",
        "    total_patients = 0\n",
        "    total_bp_sum = 0\n",
        "\n",
        "    for chunk in read_and_filter(filepath):\n",
        "        total_patients += len(chunk)\n",
        "        total_bp_sum += chunk[\"systolic_bp\"].sum()\n",
        "\n",
        "    if total_patients > 0:\n",
        "        return total_bp_sum / total_patients\n",
        "    return None\n",
        "\n",
        "mean_bp = process_elderly_patients(\"data/large_patient_file.csv\")\n",
        "print(f\"Mean systolic BP for patients over 65: {mean_bp:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b1ec38",
      "metadata": {
        "id": "04b1ec38"
      },
      "source": [
        "### Writing Results in Chunks\n",
        "\n",
        "When your output is also large, write incrementally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2fea7d6",
      "metadata": {
        "id": "e2fea7d6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def transform_and_save(input_path, output_path, chunksize=50000):\n",
        "    \"\"\"Transform a large file and save results incrementally.\"\"\"\n",
        "    first_chunk = True\n",
        "\n",
        "    for chunk in pd.read_csv(input_path, chunksize=chunksize):\n",
        "        # Apply transformations\n",
        "        chunk[\"age_group\"] = pd.cut(chunk[\"age\"], bins=[0, 18, 40, 65, 100],\n",
        "                                    labels=[\"child\", \"young_adult\", \"middle_age\", \"senior\"])\n",
        "        chunk[\"bp_category\"] = pd.cut(chunk[\"systolic_bp\"], bins=[0, 120, 140, 200],\n",
        "                                      labels=[\"normal\", \"elevated\", \"high\"])\n",
        "\n",
        "        # Write to output file\n",
        "        chunk.to_csv(\n",
        "            output_path,\n",
        "            mode=\"w\" if first_chunk else \"a\",\n",
        "            header=first_chunk,\n",
        "            index=False\n",
        "        )\n",
        "        first_chunk = False\n",
        "\n",
        "transform_and_save(\"data/raw_patients.csv\", \"data/processed_patients.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1918e955",
      "metadata": {
        "id": "1918e955"
      },
      "source": [
        "### Question\n",
        "\n",
        "You have a 50 GB CSV file containing clinical trial data with columns: `patient_id`, `visit_date`, `measurement_type`, `value`. You need to compute the mean value for each measurement type. The file has 500 million rows and 100 unique measurement types.\n",
        "\n",
        "Describe how you would approach this task using chunked processing. What data structures would you use to accumulate results? How would you compute the final means?\n",
        "\n",
        "#### Answer\n",
        "\n",
        "The approach uses two accumulators per measurement type: running sums and counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec3f360c",
      "metadata": {
        "id": "ec3f360c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def compute_measurement_means(filepath, chunksize=100000):\n",
        "    # Accumulators: sum and count for each measurement type\n",
        "    sums = defaultdict(float)\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    for chunk in pd.read_csv(filepath, chunksize=chunksize,\n",
        "                             usecols=[\"measurement_type\", \"value\"],\n",
        "                             dtype={\"measurement_type\": \"category\", \"value\": \"float32\"}):\n",
        "        # Aggregate within chunk\n",
        "        grouped = chunk.groupby(\"measurement_type\")[\"value\"]\n",
        "\n",
        "        for mtype, values in grouped:\n",
        "            sums[mtype] += values.sum()\n",
        "            counts[mtype] += len(values)\n",
        "\n",
        "    # Compute final means\n",
        "    return {mtype: sums[mtype] / counts[mtype] for mtype in sums}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412706c7",
      "metadata": {
        "id": "412706c7"
      },
      "source": [
        "Key points:\n",
        "- Use `usecols` to only load needed columns (ignore `patient_id`, `visit_date`)\n",
        "- Use `category` dtype for measurement_type (100 unique values)\n",
        "- Use `float32` instead of `float64` to reduce memory\n",
        "- `defaultdict` avoids key existence checks\n",
        "- Only 100 measurement types means accumulators use negligible memory\n",
        "- Each chunk processes independently, memory stays constant regardless of file size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39554319",
      "metadata": {
        "id": "39554319"
      },
      "source": [
        "## Efficient File Formats\n",
        "\n",
        "CSV is human-readable but inefficient for large data. Binary formats offer significant improvements in size, speed, and functionality.\n",
        "\n",
        "### CSV Limitations\n",
        "\n",
        "CSV files have several drawbacks for large data:\n",
        "\n",
        "- **No type information**: All data is stored as text and must be parsed\n",
        "- **No compression**: Files are larger than necessary\n",
        "- **Row-oriented**: Reading a single column requires scanning the entire file\n",
        "- **Slow parsing**: Text parsing is computationally expensive\n",
        "\n",
        "### Parquet: Columnar Storage\n",
        "\n",
        "Parquet is a columnar format designed for analytics. It's the standard for big data ecosystems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f170615",
      "metadata": {
        "id": "8f170615"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Write to Parquet (requires pyarrow: pip install pyarrow)\n",
        "df.to_parquet(\"data/patients.parquet\")\n",
        "\n",
        "# Read from Parquet\n",
        "df = pd.read_parquet(\"data/patients.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0477c14b",
      "metadata": {
        "id": "0477c14b"
      },
      "source": [
        "Parquet advantages:\n",
        "\n",
        "- **Columnar storage**: Reading a subset of columns is fast\n",
        "- **Built-in compression**: Typically 2-10x smaller than CSV\n",
        "- **Type preservation**: Data types are stored in the file\n",
        "- **Fast reads**: No text parsing needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e8af933",
      "metadata": {
        "id": "6e8af933",
        "outputId": "ea0d233c-3b01-4b63-cb0e-6d6c3d11c867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV: 18.01 MB\n",
            "Parquet: 4.09 MB\n",
            "Compression ratio: 4.4x\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create a sample DataFrame\n",
        "n = 1000000\n",
        "df = pd.DataFrame({\n",
        "    \"patient_id\": range(n),\n",
        "    \"age\": [45] * n,\n",
        "    \"diagnosis\": [\"diabetes\"] * n\n",
        "})\n",
        "\n",
        "# Compare file sizes\n",
        "df.to_csv(\"data/test.csv\", index=False)\n",
        "df.to_parquet(\"data/test.parquet\")\n",
        "\n",
        "csv_size = os.path.getsize(\"data/test.csv\") / 1024**2\n",
        "parquet_size = os.path.getsize(\"data/test.parquet\") / 1024**2\n",
        "print(f\"CSV: {csv_size:.2f} MB\")\n",
        "print(f\"Parquet: {parquet_size:.2f} MB\")\n",
        "print(f\"Compression ratio: {csv_size/parquet_size:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e73c84b9",
      "metadata": {
        "id": "e73c84b9"
      },
      "source": [
        "### Reading Column Subsets from Parquet\n",
        "\n",
        "One major advantage of columnar storage is efficient column selection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99630e85",
      "metadata": {
        "id": "99630e85"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Only read two columns from a file with many columns\n",
        "df = pd.read_parquet(\"data/expression_matrix.parquet\", columns=[\"gene_symbol\", \"sample_001\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad82f4df",
      "metadata": {
        "id": "ad82f4df"
      },
      "source": [
        "This reads only the requested columns from disk, unlike CSV where the entire row must be scanned.\n",
        "\n",
        "### Compression Options\n",
        "\n",
        "Parquet supports several compression algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b41bb8",
      "metadata": {
        "id": "77b41bb8"
      },
      "outputs": [],
      "source": [
        "# Default: snappy (fast, moderate compression)\n",
        "df.to_parquet(\"data/data_snappy.parquet\", compression=\"snappy\")\n",
        "\n",
        "# Best compression ratio\n",
        "df.to_parquet(\"data/data_gzip.parquet\", compression=\"gzip\")\n",
        "\n",
        "# Fast compression\n",
        "df.to_parquet(\"data/data_lz4.parquet\", compression=\"lz4\")\n",
        "\n",
        "# No compression\n",
        "df.to_parquet(\"data/data_none.parquet\", compression=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bbc87b5",
      "metadata": {
        "id": "5bbc87b5"
      },
      "source": [
        "Choose based on your needs:\n",
        "- **snappy**: Good default, balanced speed and size\n",
        "- **gzip**: Smallest files, slower read/write\n",
        "- **lz4**: Fastest, larger files than snappy\n",
        "\n",
        "### Question\n",
        "\n",
        "You're designing storage for a gene expression study with:\n",
        "- 50,000 genes (rows)\n",
        "- 10,000 samples (columns)\n",
        "- Expression values are floats\n",
        "- You frequently need to access all genes for a single sample\n",
        "- You occasionally need to access a single gene across all samples\n",
        "- Storage space is limited\n",
        "\n",
        "What file format and layout would you choose? How would you organize the data?\n",
        "\n",
        "#### Answer\n",
        "\n",
        "This requires understanding how columnar vs row-oriented storage works:\n",
        "\n",
        "**Columnar storage (Parquet)** stores each column contiguously on disk. Reading one column is fast (single contiguous read), but reading one row requires accessing every column file.\n",
        "\n",
        "**Row-oriented storage (CSV)** stores each row contiguously. Reading one row is fast, but reading one column requires scanning the entire file.\n",
        "\n",
        "Given the access patterns:\n",
        "- **Frequent**: all genes for one sample → need to read one \"sample column\"\n",
        "- **Occasional**: one gene across all samples → need to read one \"gene row\"\n",
        "\n",
        "**Recommendation: Parquet with samples as columns** (genes as rows)\n",
        "\n",
        "In this layout:\n",
        "- Each sample is one column → reading all 50,000 genes for sample_001 reads one contiguous column = **fast**\n",
        "- Each gene is one row → reading TP53 across all 10,000 samples requires accessing all column files = **slower, but acceptable for occasional use**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba02662",
      "metadata": {
        "id": "3ba02662"
      },
      "outputs": [],
      "source": [
        "# Organize as: rows = genes, columns = samples\n",
        "expression_df.to_parquet(\n",
        "    \"data/expression.parquet\",\n",
        "    compression=\"snappy\"  # Good balance of speed and size\n",
        ")\n",
        "\n",
        "# Fast: get all genes for sample_001\n",
        "sample_data = pd.read_parquet(\"data/expression.parquet\", columns=[\"gene_symbol\", \"sample_001\"])\n",
        "\n",
        "# Slower but possible: get one gene across all samples\n",
        "all_data = pd.read_parquet(\"data/expression.parquet\")\n",
        "gene_data = all_data[all_data[\"gene_symbol\"] == \"TP53\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68da2bda",
      "metadata": {
        "id": "68da2bda"
      },
      "source": [
        "For truly large datasets where both access patterns matter, consider HDF5 with chunking aligned to your access patterns, or storing two versions (transposed) if storage permits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3097b89a",
      "metadata": {
        "id": "3097b89a"
      },
      "source": [
        "## HDF5 with h5py\n",
        "\n",
        "HDF5 (Hierarchical Data Format version 5) is designed for storing and managing large amounts of scientific data. It's particularly useful for multi-dimensional arrays and complex data hierarchies.\n",
        "\n",
        "### Why HDF5?\n",
        "\n",
        "HDF5 offers several features valuable for scientific computing:\n",
        "\n",
        "- **Hierarchical structure**: Organize data like a filesystem within a single file\n",
        "- **Partial I/O**: Read slices of large arrays without loading everything\n",
        "- **Compression**: Built-in support for various compression algorithms\n",
        "- **Self-describing**: Metadata stored alongside data\n",
        "- **Language-agnostic**: Files readable by R, MATLAB, Julia, etc.\n",
        "\n",
        "### Creating HDF5 Files\n",
        "\n",
        "The `h5py` library provides a Pythonic interface to HDF5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a720d64c",
      "metadata": {
        "id": "a720d64c"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Define dimensions as variables for flexibility\n",
        "n_genes = 20000\n",
        "n_samples = 500\n",
        "\n",
        "# Create a new HDF5 file\n",
        "with h5py.File(\"data/study_data.h5\", \"w\") as f:\n",
        "    # Create a dataset\n",
        "    expression = np.random.rand(n_genes, n_samples).astype(np.float32)\n",
        "    f.create_dataset(\"expression\", data=expression)\n",
        "\n",
        "    # Create a group (like a folder)\n",
        "    metadata = f.create_group(\"metadata\")\n",
        "\n",
        "    # Add datasets to the group\n",
        "    gene_names = np.array([f\"GENE_{i}\" for i in range(n_genes)], dtype=\"S20\")\n",
        "    metadata.create_dataset(\"gene_names\", data=gene_names)\n",
        "\n",
        "    sample_ids = np.array([f\"SAMPLE_{i}\" for i in range(n_samples)], dtype=\"S20\")\n",
        "    metadata.create_dataset(\"sample_ids\", data=sample_ids)\n",
        "\n",
        "    # Add attributes (metadata)\n",
        "    f.attrs[\"study_name\"] = \"TCGA Breast Cancer\"\n",
        "    f.attrs[\"creation_date\"] = \"2024-01-15\"\n",
        "    f.attrs[\"n_genes\"] = n_genes\n",
        "    f.attrs[\"n_samples\"] = n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e56df65",
      "metadata": {
        "id": "5e56df65"
      },
      "source": [
        "### Reading HDF5 Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c563c469",
      "metadata": {
        "id": "c563c469",
        "outputId": "64d466e4-d828-40be-b856-7b04cd6f4497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents: ['expression', 'metadata']\n",
            "Study: TCGA Breast Cancer\n",
            "Shape: (20000, 500)\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "\n",
        "with h5py.File(\"data/study_data.h5\", \"r\") as f:\n",
        "    # List contents (like ls)\n",
        "    print(\"Contents:\", list(f.keys()))\n",
        "\n",
        "    # Access attributes\n",
        "    print(\"Study:\", f.attrs[\"study_name\"])\n",
        "\n",
        "    # Read entire dataset\n",
        "    expression = f[\"expression\"][:]\n",
        "    print(\"Shape:\", expression.shape)\n",
        "\n",
        "    # Read from nested groups\n",
        "    gene_names = f[\"metadata/gene_names\"][:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc5bf8c0",
      "metadata": {
        "id": "cc5bf8c0"
      },
      "source": [
        "### Partial Reads: The Key Advantage\n",
        "\n",
        "HDF5 allows reading slices without loading the entire dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2271a1bb",
      "metadata": {
        "id": "2271a1bb",
        "outputId": "04881116-fe2f-4806-89fa-6f03f3f83375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subset shape: (100, 10)\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "\n",
        "with h5py.File(\"data/study_data.h5\", \"r\") as f:\n",
        "    # Read only first 100 genes for first 10 samples\n",
        "    # This reads only this slice from disk, not the full array\n",
        "    subset = f[\"expression\"][:100, :10]\n",
        "    print(\"Subset shape:\", subset.shape)\n",
        "\n",
        "    # Read a single gene across all samples\n",
        "    gene_42 = f[\"expression\"][42, :]\n",
        "\n",
        "    # Read expression for genes 1000-2000, samples 100-200\n",
        "    region = f[\"expression\"][1000:2000, 100:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe9ccfa",
      "metadata": {
        "id": "afe9ccfa"
      },
      "source": [
        "This is possible because HDF5 stores data in chunks and tracks chunk locations.\n",
        "\n",
        "### Chunking for Efficient Access\n",
        "\n",
        "Chunking divides the dataset into fixed-size blocks. Choose chunk shape based on access patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9d7aa7c",
      "metadata": {
        "id": "f9d7aa7c"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File(\"data/chunked_data.h5\", \"w\") as f:\n",
        "    # Create dataset with explicit chunking\n",
        "    # If we typically access rows (genes), chunk by rows\n",
        "    f.create_dataset(\n",
        "        \"expression\",\n",
        "        shape=(20000, 500),\n",
        "        dtype=\"float32\",\n",
        "        chunks=(100, 500)  # Each chunk is 100 genes x all samples\n",
        "    )\n",
        "\n",
        "    # Fill with data\n",
        "    data = np.random.rand(20000, 500).astype(np.float32)\n",
        "    f[\"expression\"][:] = data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d809404",
      "metadata": {
        "id": "3d809404"
      },
      "source": [
        "Access patterns and chunking:\n",
        "- Access full rows frequently: `chunks=(small, full_width)`\n",
        "- Access full columns frequently: `chunks=(full_height, small)`\n",
        "- Random access: `chunks=(moderate, moderate)`\n",
        "\n",
        "### Compression in HDF5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6805abb2",
      "metadata": {
        "id": "6805abb2"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File(\"data/compressed_data.h5\", \"w\") as f:\n",
        "    data = np.random.rand(20000, 500).astype(np.float32)\n",
        "\n",
        "    # gzip compression (most portable)\n",
        "    f.create_dataset(\n",
        "        \"expression_gzip\",\n",
        "        data=data,\n",
        "        compression=\"gzip\",\n",
        "        compression_opts=4  # 0-9, higher = more compression\n",
        "    )\n",
        "\n",
        "    # lzf compression (faster, less compression)\n",
        "    f.create_dataset(\n",
        "        \"expression_lzf\",\n",
        "        data=data,\n",
        "        compression=\"lzf\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c544f3f3",
      "metadata": {
        "id": "c544f3f3"
      },
      "source": [
        "### Resizable Datasets\n",
        "\n",
        "When you don't know the final size in advance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ae92aa",
      "metadata": {
        "id": "f6ae92aa",
        "outputId": "22dcdcab-5004-47b3-ebcb-94c02bde8251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final shape: (5000, 10)\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File(\"data/growing_data.h5\", \"w\") as f:\n",
        "    # Create resizable dataset\n",
        "    # maxshape=None means unlimited size in that dimension\n",
        "    f.create_dataset(\n",
        "        \"measurements\",\n",
        "        shape=(0, 10),  # Start empty\n",
        "        maxshape=(None, 10),  # Can grow in first dimension\n",
        "        dtype=\"float32\",\n",
        "        chunks=(1000, 10)\n",
        "    )\n",
        "\n",
        "    # Add data in batches\n",
        "    for batch in range(5):\n",
        "        new_data = np.random.rand(1000, 10).astype(np.float32)\n",
        "\n",
        "        # Resize and append\n",
        "        current_size = f[\"measurements\"].shape[0]\n",
        "        f[\"measurements\"].resize(current_size + 1000, axis=0)\n",
        "        f[\"measurements\"][current_size:current_size + 1000] = new_data\n",
        "\n",
        "    print(\"Final shape:\", f[\"measurements\"].shape)  # (5000, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd93a363",
      "metadata": {
        "id": "fd93a363"
      },
      "source": [
        "### Question\n",
        "\n",
        "You're designing storage for a longitudinal multi-omics study with:\n",
        "- 1000 patients\n",
        "- 5 time points per patient\n",
        "- 3 data types: genomics (1M variants), proteomics (5000 proteins), metabolomics (500 metabolites)\n",
        "- Each measurement is a single float value\n",
        "\n",
        "Sketch an HDF5 structure for this data. Consider: How would you organize the hierarchy? What chunking strategy would you use for each data type?\n",
        "\n",
        "#### Answer\n",
        "\n",
        "A hierarchical structure mirrors the study design:\n",
        "\n",
        "```\n",
        "study.h5\n",
        "├── metadata/\n",
        "│   ├── patient_ids          # (1000,) array of strings\n",
        "│   ├── time_points          # (5,) array: [0, 6, 12, 18, 24] months\n",
        "│   ├── variant_ids          # (1000000,) array of strings\n",
        "│   ├── protein_ids          # (5000,) array of strings\n",
        "│   └── metabolite_ids       # (500,) array of strings\n",
        "│\n",
        "├── genomics/                # 1M variants x 1000 patients x 5 timepoints\n",
        "│   └── variants             # shape (1000000, 1000, 5), float32\n",
        "│                            # chunks (10000, 100, 5) - access by variant\n",
        "│\n",
        "├── proteomics/              # 5000 proteins x 1000 patients x 5 timepoints\n",
        "│   └── proteins             # shape (5000, 1000, 5), float32\n",
        "│                            # chunks (500, 100, 5) - access by protein\n",
        "│\n",
        "└── metabolomics/            # 500 metabolites x 1000 patients x 5 timepoints\n",
        "    └── metabolites          # shape (500, 1000, 5), float32\n",
        "                             # chunks (500, 100, 5) - small enough for full feature access\n",
        "```\n",
        "\n",
        "Chunking rationale:\n",
        "- **Genomics**: Large feature dimension, chunk to enable efficient access to subsets of variants\n",
        "- **Proteomics/Metabolomics**: Smaller feature dimensions, can chunk more aggressively on patient dimension\n",
        "- All include full time dimension in chunks since longitudinal analysis is common"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32e4152",
      "metadata": {
        "id": "d32e4152"
      },
      "source": [
        "## SQL Databases with SQLite\n",
        "\n",
        "For tabular data with complex queries, relational databases offer advantages over flat files. SQLite is a lightweight database that requires no server setup.\n",
        "\n",
        "### When to Use a Database\n",
        "\n",
        "Databases excel when you need to:\n",
        "- Query subsets of data without loading everything\n",
        "- Join related tables\n",
        "- Update or delete specific records\n",
        "- Enforce data integrity (unique IDs, required fields)\n",
        "- Share data with non-Python tools\n",
        "\n",
        "### SQL Basics\n",
        "\n",
        "SQL (Structured Query Language) is the standard language for interacting with relational databases. Here are the key commands we'll use:\n",
        "\n",
        "**Creating tables:**\n",
        "- `CREATE TABLE` defines a new table with column names and data types\n",
        "- `PRIMARY KEY` uniquely identifies each row (like patient_id)\n",
        "- `FOREIGN KEY` links to another table's primary key (for relationships)\n",
        "\n",
        "**Data types in SQLite:**\n",
        "- `TEXT` - strings\n",
        "- `INTEGER` - whole numbers  \n",
        "- `REAL` - floating-point numbers\n",
        "- `BLOB` - binary data\n",
        "\n",
        "### Creating a SQLite Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cf4fa6",
      "metadata": {
        "id": "33cf4fa6"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Create connection (creates file if it doesn't exist)\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "\n",
        "# CREATE TABLE defines a new table\n",
        "# Column format: column_name DATA_TYPE [constraints]\n",
        "# PRIMARY KEY: ensures each patient_id is unique and not null\n",
        "conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS patients (\n",
        "        patient_id TEXT PRIMARY KEY,\n",
        "        age INTEGER,\n",
        "        sex TEXT,\n",
        "        diagnosis TEXT,\n",
        "        enrollment_date TEXT\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# FOREIGN KEY links lab_results.patient_id to patients.patient_id\n",
        "# This ensures we can't add lab results for non-existent patients\n",
        "# AUTOINCREMENT automatically assigns increasing IDs to new rows\n",
        "conn.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS lab_results (\n",
        "        result_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        patient_id TEXT,\n",
        "        test_date TEXT,\n",
        "        test_name TEXT,\n",
        "        value REAL,\n",
        "        FOREIGN KEY (patient_id) REFERENCES patients(patient_id)\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7114fda3",
      "metadata": {
        "id": "7114fda3"
      },
      "source": [
        "### pandas Integration\n",
        "\n",
        "pandas provides seamless reading and writing to SQL databases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62dfc8b7",
      "metadata": {
        "id": "62dfc8b7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "\n",
        "# Create sample data\n",
        "patients = pd.DataFrame({\n",
        "    \"patient_id\": [f\"P{i:04d}\" for i in range(10000)],\n",
        "    \"age\": np.random.randint(18, 90, 10000),\n",
        "    \"sex\": np.random.choice([\"M\", \"F\"], 10000),\n",
        "    \"diagnosis\": np.random.choice([\"healthy\", \"diabetes\", \"cancer\", \"cardiovascular\"], 10000),\n",
        "    \"enrollment_date\": pd.date_range(\"2020-01-01\", periods=10000, freq=\"h\").astype(str)\n",
        "})\n",
        "\n",
        "# Write to database\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "patients.to_sql(\"patients\", conn, if_exists=\"replace\", index=False)\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327468da",
      "metadata": {
        "id": "327468da"
      },
      "source": [
        "### Querying with SQL\n",
        "\n",
        "The power of databases is efficient querying. The basic query structure is:\n",
        "\n",
        "```sql\n",
        "SELECT columns FROM table WHERE condition\n",
        "```\n",
        "\n",
        "- `SELECT` specifies which columns to retrieve (`*` means all columns)\n",
        "- `FROM` specifies the table\n",
        "- `WHERE` filters rows based on conditions\n",
        "- `GROUP BY` groups rows for aggregation\n",
        "- `AVG()`, `COUNT()`, `SUM()` are aggregate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63cf72bc",
      "metadata": {
        "id": "63cf72bc",
        "outputId": "9d601c52-328b-45a1-d18e-78dae0be4dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        diagnosis  count   mean_age\n",
            "0          cancer   2593  54.303509\n",
            "1  cardiovascular   2537  53.305085\n",
            "2        diabetes   2407  53.288741\n",
            "3         healthy   2463  53.691839\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "\n",
        "# SELECT * returns all columns from the patients table\n",
        "df = pd.read_sql(\"SELECT * FROM patients\", conn)\n",
        "\n",
        "# WHERE clause filters rows\n",
        "# AND combines multiple conditions (both must be true)\n",
        "elderly_diabetics = pd.read_sql(\"\"\"\n",
        "    SELECT * FROM patients\n",
        "    WHERE age > 65 AND diagnosis = 'diabetes'\n",
        "\"\"\", conn)\n",
        "\n",
        "# GROUP BY groups rows with same diagnosis value\n",
        "# Aggregate functions compute one value per group:\n",
        "#   COUNT(*) counts rows in each group\n",
        "#   AVG(age) computes mean age for each group\n",
        "diagnosis_counts = pd.read_sql(\"\"\"\n",
        "    SELECT diagnosis, COUNT(*) as count, AVG(age) as mean_age\n",
        "    FROM patients\n",
        "    GROUP BY diagnosis\n",
        "\"\"\", conn)\n",
        "\n",
        "print(diagnosis_counts)\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7732e7c",
      "metadata": {
        "id": "e7732e7c"
      },
      "source": [
        "### Parameterized Queries\n",
        "\n",
        "Never build SQL strings with f-strings or concatenation. This creates a security vulnerability called **SQL injection**, where malicious input can execute unintended commands.\n",
        "\n",
        "For example, if a user enters `'; DROP TABLE patients; --` as their diagnosis, a naive query could delete your entire table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0498889c",
      "metadata": {
        "id": "0498889c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "\n",
        "# Safe: parameterized query using ? placeholders\n",
        "# The database driver escapes special characters in the parameters\n",
        "min_age = 65\n",
        "diagnosis = \"diabetes\"\n",
        "\n",
        "df = pd.read_sql(\n",
        "    \"SELECT * FROM patients WHERE age > ? AND diagnosis = ?\",\n",
        "    conn,\n",
        "    params=(min_age, diagnosis)\n",
        ")\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j88hsqgmpxj",
      "metadata": {
        "id": "j88hsqgmpxj"
      },
      "source": [
        "Always sanitize your inputs and use parameterized queries to prevent this!\n",
        "\n",
        "![Bobby Tables](https://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n",
        "\n",
        "[xkcd.com/327](https://xkcd.com/327/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f39aa7",
      "metadata": {
        "id": "e9f39aa7"
      },
      "source": [
        "### Joining Tables\n",
        "\n",
        "`JOIN` combines rows from two tables based on a related column. This is one of the most powerful features of relational databases.\n",
        "\n",
        "```sql\n",
        "SELECT columns\n",
        "FROM table1\n",
        "JOIN table2 ON table1.key = table2.key\n",
        "```\n",
        "\n",
        "The `ON` clause specifies how to match rows between tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc60dc88",
      "metadata": {
        "id": "cc60dc88"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "\n",
        "# JOIN combines patients (p) with their lab_results (l)\n",
        "# ON specifies the matching column between tables\n",
        "# p. and l. are aliases for table names (shorter to type)\n",
        "# This query finds HbA1c values for diabetic patients\n",
        "query = \"\"\"\n",
        "    SELECT p.patient_id, p.age, p.diagnosis,\n",
        "           l.test_name, l.value, l.test_date\n",
        "    FROM patients p\n",
        "    JOIN lab_results l ON p.patient_id = l.patient_id\n",
        "    WHERE p.diagnosis = 'diabetes'\n",
        "      AND l.test_name = 'HbA1c'\n",
        "\"\"\"\n",
        "\n",
        "diabetic_hba1c = pd.read_sql(query, conn)\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74001b0b",
      "metadata": {
        "id": "74001b0b"
      },
      "source": [
        "### Indexing for Performance\n",
        "\n",
        "For large tables, indexes dramatically speed up queries. An index is like a book's index: it lets the database quickly find rows matching a condition without scanning every row.\n",
        "\n",
        "Create indexes on columns you frequently filter or join on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8510ae",
      "metadata": {
        "id": "0f8510ae"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "\n",
        "# CREATE INDEX creates a lookup structure for faster queries\n",
        "# idx_diagnosis speeds up: WHERE diagnosis = '...'\n",
        "# idx_patient_lab speeds up: JOIN ... ON patient_id = ...\n",
        "conn.execute(\"CREATE INDEX IF NOT EXISTS idx_diagnosis ON patients(diagnosis)\")\n",
        "conn.execute(\"CREATE INDEX IF NOT EXISTS idx_patient_lab ON lab_results(patient_id)\")\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c3d6b48",
      "metadata": {
        "id": "0c3d6b48"
      },
      "source": [
        "### Question\n",
        "\n",
        "You have a SQLite database with tables `patients` (patient_id, age, sex, site) and `visits` (visit_id, patient_id, visit_date, systolic_bp, diastolic_bp). Write a SQL query to find the mean systolic blood pressure for female patients over 60, grouped by site.\n",
        "\n",
        "#### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24b0fbac-0548-4c1c-9ca8-0162fd1cb021",
      "metadata": {
        "id": "24b0fbac-0548-4c1c-9ca8-0162fd1cb021"
      },
      "source": [
        "%%sql\n",
        "SELECT p.site, AVG(v.systolic_bp) as mean_systolic_bp, COUNT(*) as n_visits\n",
        "FROM patients p\n",
        "JOIN visits v ON p.patient_id = v.patient_id\n",
        "WHERE p.sex = 'F' AND p.age > 60\n",
        "GROUP BY p.site\n",
        "ORDER BY mean_systolic_bp DESC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec023091",
      "metadata": {
        "id": "ec023091"
      },
      "source": [
        "In pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6331b982",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6331b982",
        "outputId": "d5837977-a809-4fb1-d73f-a877f2499943"
      },
      "outputs": [
        {
          "ename": "DatabaseError",
          "evalue": "Execution failed on sql '\n    SELECT p.site, AVG(v.systolic_bp) as mean_systolic_bp, COUNT(*) as n_visits\n    FROM patients p\n    JOIN visits v ON p.patient_id = v.patient_id\n    WHERE p.sex = 'F' AND p.age > 60\n    GROUP BY p.site\n    ORDER BY mean_systolic_bp DESC\n': no such table: visits",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "File \u001b[0;32m~/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages/pandas/io/sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: visits",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[1;32m      4\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/clinical_study.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m    SELECT p.site, AVG(v.systolic_bp) as mean_systolic_bp, COUNT(*) as n_visits\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m    FROM patients p\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    JOIN visits v ON p.patient_id = v.patient_id\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m    WHERE p.sex = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m AND p.age > 60\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m    GROUP BY p.site\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    ORDER BY mean_systolic_bp DESC\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages/pandas/io/sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages/pandas/io/sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2729\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.5/envs/experiment/lib/python3.12/site-packages/pandas/io/sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\n    SELECT p.site, AVG(v.systolic_bp) as mean_systolic_bp, COUNT(*) as n_visits\n    FROM patients p\n    JOIN visits v ON p.patient_id = v.patient_id\n    WHERE p.sex = 'F' AND p.age > 60\n    GROUP BY p.site\n    ORDER BY mean_systolic_bp DESC\n': no such table: visits"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"data/clinical_study.db\")\n",
        "result = pd.read_sql(\"\"\"\n",
        "    SELECT p.site, AVG(v.systolic_bp) as mean_systolic_bp, COUNT(*) as n_visits\n",
        "    FROM patients p\n",
        "    JOIN visits v ON p.patient_id = v.patient_id\n",
        "    WHERE p.sex = 'F' AND p.age > 60\n",
        "    GROUP BY p.site\n",
        "    ORDER BY mean_systolic_bp DESC\n",
        "\"\"\", conn)\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0557b806",
      "metadata": {
        "id": "0557b806"
      },
      "source": [
        "## Sparse Matrices with scipy.sparse\n",
        "\n",
        "Many biomedical datasets are sparse: most values are zero. Examples include:\n",
        "- Single-cell RNA-seq: Most genes have zero counts in most cells\n",
        "- Genetic variants: Most individuals have the reference allele at most positions\n",
        "- Document-term matrices: Most documents don't contain most words\n",
        "- Network adjacency matrices: Most nodes aren't connected\n",
        "\n",
        "### The Memory Problem with Dense Storage\n",
        "\n",
        "Consider a single-cell gene expression matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2dbf65",
      "metadata": {
        "id": "6b2dbf65"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "n_genes = 30000\n",
        "n_cells = 100000\n",
        "\n",
        "# Dense storage (even if 95% zeros)\n",
        "# Memory: 30000 * 100000 * 8 bytes = 24 GB\n",
        "dense_matrix = np.zeros((n_genes, n_cells), dtype=np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989c3eb3",
      "metadata": {
        "id": "989c3eb3"
      },
      "source": [
        "Even storing zeros uses 8 bytes each. For a 95% sparse matrix, we're wasting 95% of memory.\n",
        "\n",
        "### Sparse Matrix Formats\n",
        "\n",
        "scipy.sparse provides several formats optimized for different operations:\n",
        "\n",
        "**COO (Coordinate)**: Stores (row, col, value) triplets\n",
        "- Good for: Construction, converting to other formats\n",
        "- Bad for: Arithmetic, slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b3ceeb",
      "metadata": {
        "id": "14b3ceeb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "# Create from coordinate lists\n",
        "rows = np.array([0, 0, 1, 2, 2])\n",
        "cols = np.array([0, 2, 1, 0, 2])\n",
        "data = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "\n",
        "sparse_coo = coo_matrix((data, (rows, cols)), shape=(3, 3))\n",
        "print(sparse_coo.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193d9488",
      "metadata": {
        "id": "193d9488"
      },
      "source": [
        "Output:\n",
        "```\n",
        "[[1. 0. 2.]\n",
        " [0. 3. 0.]\n",
        " [4. 0. 5.]]\n",
        "```\n",
        "\n",
        "**CSR (Compressed Sparse Row)**: Stores data row-by-row\n",
        "- Good for: Row slicing, matrix-vector products\n",
        "- Standard format for machine learning\n",
        "\n",
        "CSR uses three arrays:\n",
        "- `data`: non-zero values, stored row-by-row: `[1, 2, 3, 4, 5]`\n",
        "- `indices`: column index of each value: `[0, 2, 1, 0, 2]`\n",
        "- `indptr`: where each row starts in data array: `[0, 2, 3, 5]`\n",
        "  - Row 0 spans data[0:2], Row 1 spans data[2:3], Row 2 spans data[3:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65699592",
      "metadata": {
        "id": "65699592"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "sparse_csr = coo_matrix((data, (rows, cols)), shape=(3, 3)).tocsr()\n",
        "\n",
        "# Efficient row access\n",
        "row_1 = sparse_csr[1, :]  # Fast\n",
        "print(\"Row 1:\", row_1.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0887f08d",
      "metadata": {
        "id": "0887f08d"
      },
      "source": [
        "**CSC (Compressed Sparse Column)**: Stores data column-by-column\n",
        "- Good for: Column slicing, solving linear systems\n",
        "\n",
        "CSC is like CSR but organized by columns instead of rows:\n",
        "- `data`: non-zero values, stored column-by-column\n",
        "- `indices`: row index of each value\n",
        "- `indptr`: where each column starts in data array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2331487",
      "metadata": {
        "id": "f2331487"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csc_matrix\n",
        "\n",
        "sparse_csc = coo_matrix((data, (rows, cols)), shape=(3, 3)).tocsc()\n",
        "\n",
        "# Efficient column access\n",
        "col_2 = sparse_csc[:, 2]  # Fast\n",
        "print(\"Col 2:\", col_2.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9298704c",
      "metadata": {
        "id": "9298704c"
      },
      "source": [
        "### Memory Comparison\n",
        "\n",
        "Let's compare the memory usage of dense vs sparse storage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e308ebc",
      "metadata": {
        "id": "7e308ebc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import sys\n",
        "\n",
        "# Create a sparse matrix (95% zeros)\n",
        "n = 10000\n",
        "density = 0.05  # 5% non-zero\n",
        "\n",
        "np.random.seed(42)\n",
        "dense = np.zeros((n, n))\n",
        "n_nonzero = int(n * n * density)\n",
        "rows = np.random.randint(0, n, n_nonzero)\n",
        "cols = np.random.randint(0, n, n_nonzero)\n",
        "dense[rows, cols] = np.random.rand(n_nonzero)\n",
        "\n",
        "sparse = csr_matrix(dense)\n",
        "\n",
        "# Compare memory\n",
        "dense_mb = dense.nbytes / 1024**2\n",
        "sparse_mb = (sparse.data.nbytes + sparse.indices.nbytes + sparse.indptr.nbytes) / 1024**2\n",
        "\n",
        "print(f\"Dense: {dense_mb:.1f} MB\")\n",
        "print(f\"Sparse: {sparse_mb:.1f} MB\")\n",
        "print(f\"Ratio: {dense_mb/sparse_mb:.1f}x savings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995bfafc",
      "metadata": {
        "id": "995bfafc"
      },
      "source": [
        "Output:\n",
        "```\n",
        "Dense: 762.9 MB\n",
        "Sparse: 57.2 MB\n",
        "Ratio: 13.3x savings\n",
        "```\n",
        "\n",
        "### Operations on Sparse Matrices\n",
        "\n",
        "Most numpy-like operations work on sparse matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfad2dd7",
      "metadata": {
        "id": "dfad2dd7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, random\n",
        "\n",
        "# Create two sparse matrices\n",
        "A = random(1000, 1000, density=0.01, format=\"csr\")\n",
        "B = random(1000, 1000, density=0.01, format=\"csr\")\n",
        "\n",
        "# Matrix multiplication (stays sparse)\n",
        "C = A @ B\n",
        "\n",
        "# Element-wise operations\n",
        "D = A + B\n",
        "E = A.multiply(B)  # Element-wise, not @\n",
        "\n",
        "# Scalar operations\n",
        "F = A * 2.0\n",
        "\n",
        "# Statistics (may convert to dense internally for some operations)\n",
        "row_sums = np.array(A.sum(axis=1)).flatten()\n",
        "col_means = np.array(A.mean(axis=0)).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62aac25f",
      "metadata": {
        "id": "62aac25f"
      },
      "source": [
        "### Converting Between Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87e3160",
      "metadata": {
        "id": "e87e3160"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix, csr_matrix, csc_matrix\n",
        "\n",
        "# Start with COO (good for construction)\n",
        "coo = coo_matrix((data, (rows, cols)), shape=(1000, 1000))\n",
        "\n",
        "# Convert for row operations\n",
        "csr = coo.tocsr()\n",
        "\n",
        "# Convert for column operations\n",
        "csc = coo.tocsc()\n",
        "\n",
        "# Convert back to dense (careful with memory!)\n",
        "dense = csr.toarray()  # Only if matrix is small enough"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc2bb92",
      "metadata": {
        "id": "1cc2bb92"
      },
      "source": [
        "### Sparse Matrices in Practice: Single-Cell Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f26c9df3",
      "metadata": {
        "id": "f26c9df3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
        "from scipy.sparse import random\n",
        "\n",
        "# Simulate single-cell expression (highly sparse)\n",
        "n_genes = 20000\n",
        "n_cells = 50000\n",
        "density = 0.05  # 5% of gene-cell pairs have non-zero expression\n",
        "\n",
        "# Generate sparse data directly\n",
        "expression = random(n_genes, n_cells, density=density,\n",
        "                    format=\"csr\", dtype=np.float32)\n",
        "\n",
        "# Save sparse matrix\n",
        "save_npz(\"data/expression_sparse.npz\", expression)\n",
        "\n",
        "# Load sparse matrix\n",
        "expression_loaded = load_npz(\"data/expression_sparse.npz\")\n",
        "\n",
        "# Compute gene statistics efficiently\n",
        "gene_means = np.array(expression.mean(axis=1)).flatten()\n",
        "gene_nonzero_counts = np.diff(expression.indptr)  # CSR gives this for free\n",
        "\n",
        "print(f\"Shape: {expression.shape}\")\n",
        "print(f\"Non-zero elements: {expression.nnz:,}\")\n",
        "print(f\"Sparsity: {1 - expression.nnz / (n_genes * n_cells):.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d278a59a",
      "metadata": {
        "id": "d278a59a"
      },
      "source": [
        "### Question\n",
        "\n",
        "You have a gene-cell expression matrix where 97% of values are zero. The matrix has 25,000 genes and 200,000 cells. Expression values are counts ranging from 0 to 1000.\n",
        "\n",
        "1. Estimate the memory for dense vs sparse storage\n",
        "2. What sparse format would you use and why?\n",
        "3. What data type would you use for the values?\n",
        "\n",
        "#### Answer\n",
        "\n",
        "1. **Memory estimation**:\n",
        "   - Dense: 25,000 × 200,000 × 8 bytes (float64) = 40 GB\n",
        "   - Sparse (3% non-zero): ~150 million non-zero values\n",
        "     - CSR: data (150M × 4 bytes) + indices (150M × 4 bytes) + indptr (25K × 4 bytes) ≈ 1.2 GB\n",
        "   - Savings: ~33x\n",
        "\n",
        "2. **Format choice**: **CSR** (Compressed Sparse Row)\n",
        "   - Gene expression analysis typically computes statistics per gene (row-wise operations)\n",
        "   - CSR is efficient for row slicing and row-wise aggregations\n",
        "   - Also efficient for matrix-vector products common in dimensionality reduction\n",
        "\n",
        "3. **Data type**: **uint16** or **float32**\n",
        "   - Counts 0-1000 fit in uint16 (0-65535), using 2 bytes instead of 8\n",
        "   - If normalized values are needed later, float32 (4 bytes) provides sufficient precision\n",
        "   - With uint16: sparse storage drops to ~750 MB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54136b72",
      "metadata": {
        "id": "54136b72"
      },
      "source": [
        "## Brief Survey of Alternatives\n",
        "\n",
        "When pandas becomes insufficient, several libraries provide scaled-up alternatives.\n",
        "\n",
        "### Dask: Parallel pandas\n",
        "\n",
        "Dask extends pandas to larger-than-memory datasets by breaking them into partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce6ca1e",
      "metadata": {
        "id": "0ce6ca1e"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Read large CSV as Dask DataFrame\n",
        "ddf = dd.read_csv(\"data/large_patient_file.csv\")\n",
        "\n",
        "# Familiar pandas-like API\n",
        "result = ddf.groupby(\"diagnosis\")[\"age\"].mean()\n",
        "\n",
        "# Computation is lazy - nothing happens until .compute()\n",
        "print(result.compute())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d73d7b",
      "metadata": {
        "id": "78d73d7b"
      },
      "source": [
        "Dask is useful when:\n",
        "- Data is larger than memory but fits on disk\n",
        "- Operations can be parallelized across partitions\n",
        "- You want to stay close to pandas syntax\n",
        "\n",
        "### Polars: Fast DataFrames\n",
        "\n",
        "Polars is a newer library written in Rust, offering significant speed improvements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6421ec20",
      "metadata": {
        "id": "6421ec20"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Read CSV\n",
        "df = pl.read_csv(\"data/patient_records.csv\")\n",
        "\n",
        "# Eager execution (like pandas)\n",
        "result = df.filter(pl.col(\"age\") > 65).group_by(\"diagnosis\").agg(\n",
        "    pl.col(\"systolic_bp\").mean()\n",
        ")\n",
        "\n",
        "# Lazy execution (optimized query planning)\n",
        "ldf = pl.scan_csv(\"data/patient_records.csv\")  # Doesn't load data yet\n",
        "result = (\n",
        "    ldf.filter(pl.col(\"age\") > 65)\n",
        "    .group_by(\"diagnosis\")\n",
        "    .agg(pl.col(\"systolic_bp\").mean())\n",
        "    .collect()  # Executes optimized query\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3ad5ce",
      "metadata": {
        "id": "4f3ad5ce"
      },
      "source": [
        "Polars excels at:\n",
        "- Pure speed (often 10-100x faster than pandas)\n",
        "- Memory efficiency\n",
        "- Query optimization through lazy evaluation\n",
        "\n",
        "### Choosing the Right Tool\n",
        "\n",
        "| Situation | Recommended Tool |\n",
        "|-----------|------------------|\n",
        "| Data fits in memory, complex analysis | pandas (optimized dtypes) |\n",
        "| Data larger than memory, pandas-like workflow | Dask |\n",
        "| Speed-critical analysis | Polars |\n",
        "| Hierarchical scientific data, partial reads | HDF5 |\n",
        "| Relational data, complex queries | SQL database |\n",
        "| Highly sparse data | scipy.sparse |\n",
        "\n",
        "For most biomedical research, optimized pandas with efficient file formats (Parquet, HDF5) handles the majority of use cases. Move to specialized tools when you hit specific limitations.\n",
        "\n",
        "## References and Further Reading\n",
        "\n",
        "**pandas**\n",
        "- [Scaling to Large Datasets](https://pandas.pydata.org/docs/user_guide/scale.html) - Official pandas documentation on memory optimization and chunking\n",
        "\n",
        "**HDF5**\n",
        "- [h5py Documentation](https://docs.h5py.org/) - Python interface to HDF5\n",
        "- [HDF5 User Guide](https://portal.hdfgroup.org/documentation/) - Comprehensive HDF5 documentation\n",
        "\n",
        "**Sparse Matrices**\n",
        "- [scipy.sparse Tutorial](https://docs.scipy.org/doc/scipy/tutorial/sparse.html) - Official SciPy sparse matrix documentation\n",
        "\n",
        "**Alternative Libraries**\n",
        "- [Dask Documentation](https://docs.dask.org/) - Parallel computing with pandas-like API\n",
        "- [Polars User Guide](https://docs.pola.rs/) - Fast DataFrame library\n",
        "\n",
        "**SQL**\n",
        "- [SQLite Tutorial](https://www.sqlitetutorial.net/) - Learn SQL with SQLite\n",
        "- [pandas SQL Documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) - pandas-SQL integration"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}