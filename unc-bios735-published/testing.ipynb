{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d583a02c",
   "metadata": {},
   "source": [
    "# Testing Your Code\n",
    "\n",
    "## Why Testing Matters\n",
    "\n",
    "When you write code to perform statistical analyses or implement numerical methods, how do you know it works correctly? You might run a few examples by hand and check the output, but this approach has serious limitations. What happens when you change the code later? What if an edge case slips through? What if a colleague modifies a function you depend on?\n",
    "\n",
    "Testing provides a systematic way to verify that code behaves as expected. For scientific computing and statistical software, testing is especially critical:\n",
    "\n",
    "* *Correctness:* Statistical methods often have subtle requirements. An off-by-one error in a variance calculation or an incorrect normalization constant can produce plausible but wrong results.\n",
    "\n",
    "* *Reproducibility:* Tests document expected behavior. When you or others return to the code months later, tests show exactly what the code should do.\n",
    "\n",
    "* *Confidence in changes:* When you optimize a function or fix a bug, tests verify that existing behavior is preserved. This is called regression testing.\n",
    "\n",
    "* *Edge cases:* Tests force you to think about unusual inputs: empty arrays, single elements, negative numbers, or values at the boundaries of valid ranges.\n",
    "\n",
    "### Types of Testing\n",
    "\n",
    "There are several levels of testing, each serving a different purpose:\n",
    "\n",
    "**Unit tests** verify that individual functions or methods work correctly in isolation. These are the most common type of test and the focus of this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_of_single_value():\n",
    "    assert calculate_mean([5.0]) == 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d72f7d",
   "metadata": {},
   "source": [
    "**Integration tests** verify that multiple components work together correctly. For example, testing that a data loading function produces output that a model fitting function can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a812ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_and_fit_pipeline():\n",
    "    data = load_dataset(\"sample.csv\")\n",
    "    model = fit_linear_model(data[\"X\"], data[\"y\"])\n",
    "    assert model.coef_ is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c957fe",
   "metadata": {},
   "source": [
    "**Regression tests** verify that previously fixed bugs don't reappear. These are often written after discovering a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variance_with_single_element():\n",
    "    # Bug #42: variance() crashed on single-element arrays\n",
    "    result = variance([1.0])\n",
    "    assert result == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da169c58",
   "metadata": {},
   "source": [
    "## Writing Your First Test\n",
    "\n",
    "Before using any testing framework, let's understand how testing works at its core. Python's built-in `assert` statement is all you need to write basic tests.\n",
    "\n",
    "### The assert Statement\n",
    "\n",
    "The `assert` statement checks whether a condition is true. If the condition is false, Python raises an `AssertionError`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448870d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This passes silently - no output means success\n",
    "assert 2 + 2 == 4\n",
    "print(\"Assertion passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excq44ogj76",
   "metadata": {},
   "source": [
    "If we try an assertion that fails, Python raises an `AssertionError`:\n",
    "\n",
    "```python\n",
    "assert 2 + 2 == 5  # This would raise AssertionError\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "AssertionError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ba81d",
   "metadata": {},
   "source": [
    "You can add a message to make failures more informative:\n",
    "\n",
    "```python\n",
    "assert 2 + 2 == 5, \"Expected 5 but got 4\"\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "AssertionError: Expected 5 but got 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0540785",
   "metadata": {},
   "source": [
    "### A Simple Test\n",
    "\n",
    "Here's a complete example. Define a function, write a test for it, and run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "def test_mean():\n",
    "    result = calculate_mean([1, 2, 3, 4, 5])\n",
    "    assert result == 3.0, f\"Expected 3.0, got {result}\"\n",
    "\n",
    "test_mean()\n",
    "print(\"test_mean passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7thq7pqfsdt",
   "metadata": {},
   "source": [
    "Before we continue, let's define some helper functions that we'll use in examples throughout this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2c0e6",
   "metadata": {},
   "source": [
    "Save this as `test_mean.py` and run it:\n",
    "```\n",
    "python test_mean.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821cb2a-8eec-4807-918d-507c85a14208",
   "metadata": {},
   "source": [
    "If the test passes, you'll see:\n",
    "\n",
    "```\n",
    "test_mean passed\n",
    "```\n",
    "\n",
    "If the test fails (say, if `calculate_mean` had a bug), you'd see an `AssertionError` with your message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ojiblttldfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_variance(data):\n",
    "    \"\"\"Calculate population variance.\"\"\"\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    return np.mean((data - mean) ** 2)\n",
    "\n",
    "def calculate_median(data):\n",
    "    \"\"\"Calculate median of data.\"\"\"\n",
    "    sorted_data = sorted(data)\n",
    "    n = len(sorted_data)\n",
    "    if n % 2 == 1:\n",
    "        return sorted_data[n // 2]\n",
    "    return (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n",
    "\n",
    "def calculate_std(data):\n",
    "    \"\"\"Calculate population standard deviation.\"\"\"\n",
    "    return np.sqrt(calculate_variance(data))\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"Normalize data to [0, 1] range.\"\"\"\n",
    "    data = np.array(data, dtype=float)\n",
    "    min_val, max_val = np.min(data), np.max(data)\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(data)\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def compute_covariance(data):\n",
    "    \"\"\"Compute covariance matrix.\"\"\"\n",
    "    return np.cov(data, rowvar=False)\n",
    "\n",
    "def transform(X):\n",
    "    \"\"\"Simple identity transform for demonstration.\"\"\"\n",
    "    return X.copy()\n",
    "\n",
    "def remove_missing(data):\n",
    "    \"\"\"Remove None values from a list.\"\"\"\n",
    "    return [x for x in data if x is not None]\n",
    "\n",
    "def compute_stats(data):\n",
    "    \"\"\"Compute mean and std of data.\"\"\"\n",
    "    return {\"mean\": np.mean(data), \"std\": np.std(data)}\n",
    "\n",
    "def bootstrap_mean(data, n_iterations=1000):\n",
    "    \"\"\"Compute bootstrap estimate of the mean.\"\"\"\n",
    "    means = []\n",
    "    n = len(data)\n",
    "    for _ in range(n_iterations):\n",
    "        sample = data[np.random.randint(0, n, size=n)]\n",
    "        means.append(np.mean(sample))\n",
    "    return np.mean(means)\n",
    "\n",
    "def bootstrap_sample(data, rng=None):\n",
    "    \"\"\"Draw a bootstrap sample from data.\n",
    "\n",
    "    Args:\n",
    "        data: Input array.\n",
    "        rng: Random state for reproducibility. If None, uses global state.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState()\n",
    "    indices = rng.randint(0, len(data), size=len(data))\n",
    "    return data[indices]\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ede68-f791-4d6c-93f9-89011ec38faa",
   "metadata": {},
   "source": [
    "### Multiple Tests\n",
    "\n",
    "You can write multiple test functions and call them one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b35565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add_positive_numbers():\n",
    "    assert add(2, 3) == 5\n",
    "\n",
    "def test_add_negative_numbers():\n",
    "    assert add(-1, -1) == -2\n",
    "\n",
    "def test_add_mixed_numbers():\n",
    "    assert add(-1, 5) == 4\n",
    "\n",
    "# Run all tests\n",
    "test_add_positive_numbers()\n",
    "test_add_negative_numbers()\n",
    "test_add_mixed_numbers()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0776e",
   "metadata": {},
   "source": [
    "### What Makes a Good Test\n",
    "\n",
    "A good test has a clear purpose and follows a simple structure:\n",
    "\n",
    "1. Set up the input data\n",
    "2. Call the function being tested\n",
    "3. Check that the result matches expectations\n",
    "\n",
    "Each test should focus on one specific behavior. Instead of testing everything in one function, write separate tests for different cases.\n",
    "\n",
    "## The AAA Pattern\n",
    "\n",
    "Well-structured tests follow the AAA pattern: Arrange, Act, Assert. This pattern makes tests readable and maintainable.\n",
    "\n",
    "### Arrange-Act-Assert Structure\n",
    "\n",
    "* **Arrange:** Set up the test data and any required state\n",
    "* **Act:** Execute the code being tested\n",
    "* **Assert:** Verify the result is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ee775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_mean():\n",
    "    # Arrange\n",
    "    data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "    # Act\n",
    "    result = calculate_mean(data)\n",
    "\n",
    "    # Assert\n",
    "    assert result == 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800728a5",
   "metadata": {},
   "source": [
    "This structure clearly separates what you're testing from how you're testing it. When a test fails, you can quickly identify which phase caused the problem.\n",
    "\n",
    "### Writing Focused Tests\n",
    "\n",
    "Each test should verify one specific behavior. Resist the temptation to test multiple things at once.\n",
    "\n",
    "**Poor structure (multiple behaviors in one test):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_statistics():\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    assert calculate_mean(data) == 3.0\n",
    "    assert calculate_median(data) == 3.0\n",
    "    assert calculate_std(data) > 0\n",
    "    assert calculate_variance(data) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964871d1",
   "metadata": {},
   "source": [
    "If this test fails, which function is broken? You'd need to examine the error message carefully to find out.\n",
    "\n",
    "**Better structure (separate tests):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b83031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean():\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    assert calculate_mean(data) == 3.0\n",
    "\n",
    "def test_median():\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    assert calculate_median(data) == 3.0\n",
    "\n",
    "def test_std_is_positive():\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    assert calculate_std(data) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607572f",
   "metadata": {},
   "source": [
    "Each test has a single purpose and a descriptive name. When one fails, you immediately know which function has a problem.\n",
    "\n",
    "### Meaningful Test Names\n",
    "\n",
    "Test names should describe what behavior is being tested. A good pattern is `test_<function>_<scenario>_<expected_result>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05397eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_with_single_element_returns_that_element():\n",
    "    assert calculate_mean([42.0]) == 42.0\n",
    "\n",
    "def test_mean_with_negative_values_handles_correctly():\n",
    "    assert calculate_mean([-1.0, -2.0, -3.0]) == -2.0\n",
    "\n",
    "def test_normalize_with_constant_array_returns_zeros():\n",
    "    result = normalize([5.0, 5.0, 5.0])\n",
    "    assert all(x == 0.0 for x in result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f3911",
   "metadata": {},
   "source": [
    "These names serve as documentation. Reading the test names gives you a quick summary of what your code does.\n",
    "\n",
    "### One Assertion Per Test\n",
    "\n",
    "While not a strict rule, limiting to one logical assertion per test improves clarity. If you need multiple `assert` statements, they should all verify aspects of the same result.\n",
    "\n",
    "**Acceptable (checking multiple aspects of one result):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ecef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit_model_returns_correct_shape():\n",
    "    X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "    y = np.array([1, 2, 3])\n",
    "    model = fit_model(X, y)\n",
    "\n",
    "    assert model.coef_.shape == (2,)\n",
    "    assert isinstance(model.intercept_, float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba0d26",
   "metadata": {},
   "source": [
    "**Not ideal (checking unrelated behaviors):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_functionality():\n",
    "    X = np.array([[1, 2], [3, 4]])\n",
    "    y = np.array([1, 2])\n",
    "    model = fit_model(X, y)\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    assert model.coef_ is not None  # Checking fit\n",
    "    assert len(predictions) == 2    # Checking predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c98b72",
   "metadata": {},
   "source": [
    "The second example tests both `fit` and `predict` behavior. These should be separate tests.\n",
    "\n",
    "### Question\n",
    "\n",
    "The following test function violates several best practices. Identify the problems and describe how you would refactor it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_processing():\n",
    "    raw = [1, 2, None, 4, 5]\n",
    "\n",
    "    cleaned = remove_missing(raw)\n",
    "    assert len(cleaned) == 4\n",
    "\n",
    "    normalized = normalize(cleaned)\n",
    "    assert min(normalized) == 0.0\n",
    "    assert max(normalized) == 1.0\n",
    "\n",
    "    stats = compute_stats(normalized)\n",
    "    assert \"mean\" in stats\n",
    "    assert \"std\" in stats\n",
    "    assert stats[\"mean\"] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2c21e",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "This test has several problems:\n",
    "\n",
    "1. **Tests multiple functions in one test.** It tests `remove_missing`, `normalize`, and `compute_stats` together. If any function breaks, the test name doesn't indicate which one.\n",
    "\n",
    "2. **Each function's output depends on the previous function.** If `remove_missing` has a bug, the `normalize` assertions will also fail, even if `normalize` is correct.\n",
    "\n",
    "3. **The test name is too vague.** \"test_data_processing\" doesn't describe what specific behavior is being verified.\n",
    "\n",
    "Refactored version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4656466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_missing_excludes_none_values():\n",
    "    raw = [1, 2, None, 4, 5]\n",
    "    cleaned = remove_missing(raw)\n",
    "    assert len(cleaned) == 4\n",
    "    assert None not in cleaned\n",
    "\n",
    "def test_normalize_scales_to_zero_one_range():\n",
    "    data = [1.0, 2.0, 4.0, 5.0]\n",
    "    normalized = normalize(data)\n",
    "    assert min(normalized) == 0.0\n",
    "    assert max(normalized) == 1.0\n",
    "\n",
    "def test_compute_stats_returns_mean_and_std():\n",
    "    data = [0.0, 0.25, 0.5, 1.0]\n",
    "    stats = compute_stats(data)\n",
    "    assert \"mean\" in stats\n",
    "    assert \"std\" in stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6771f",
   "metadata": {},
   "source": [
    "## Testing Numerical Code\n",
    "\n",
    "Scientific computing introduces unique testing challenges. Floating-point arithmetic doesn't always produce exact results, and numerical algorithms often involve approximations.\n",
    "\n",
    "### The Floating-Point Problem\n",
    "\n",
    "Consider this simple arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a574027",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1 + 0.2  # this will give 0.30000000000000004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652efdd",
   "metadata": {},
   "source": [
    "This is not a Python bug. Binary floating-point cannot represent 0.1 exactly, just as decimal cannot represent 1/3 exactly. These small representation errors accumulate.\n",
    "\n",
    "This means a naive test will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum_no_tolerance():\n",
    "    result = 0.1 + 0.2\n",
    "    assert result == 0.3  # This will fail!\n",
    "\n",
    "# Uncomment to see the failure:\n",
    "# test_sum_no_tolerance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b49792",
   "metadata": {},
   "source": [
    "### Comparing with Tolerance\n",
    "\n",
    "To compare floating-point values, check if their difference is smaller than some tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum_with_tolerance():\n",
    "    result = 0.1 + 0.2\n",
    "    expected = 0.3\n",
    "    assert abs(result - expected) < 1e-9\n",
    "\n",
    "test_sum_with_tolerance()\n",
    "print(\"test_sum_with_tolerance passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c73d2",
   "metadata": {},
   "source": [
    "This checks that `result` is within `1e-9` of `expected`.\n",
    "\n",
    "### Using math.isclose\n",
    "\n",
    "Python's standard library provides `math.isclose()` for cleaner tolerance comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def test_sum_with_isclose():\n",
    "    result = 0.1 + 0.2\n",
    "    assert math.isclose(result, 0.3)\n",
    "\n",
    "test_sum_with_isclose()\n",
    "print(\"test_sum_with_isclose passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87895a27",
   "metadata": {},
   "source": [
    "By default, `math.isclose()` uses a relative tolerance of `1e-9`, meaning values are considered equal if they differ by less than one part per billion relative to the larger value.\n",
    "\n",
    "### Controlling Tolerance\n",
    "\n",
    "Both approaches allow you to specify tolerance explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Absolute tolerance: values within 0.001 of each other\n",
    "result = 1.001\n",
    "assert abs(result - 1.0) < 0.01  # passes\n",
    "\n",
    "# Using math.isclose with explicit tolerances\n",
    "assert math.isclose(1.0, 1.009, rel_tol=0.01)  # 1% relative tolerance\n",
    "assert math.isclose(0.001, 0.0019, abs_tol=0.001)  # absolute tolerance\n",
    "\n",
    "print(\"All tolerance examples passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428da27",
   "metadata": {},
   "source": [
    "**When to use relative vs. absolute tolerance:**\n",
    "\n",
    "* **Relative tolerance** works well for values of similar magnitude. It scales with the values being compared.\n",
    "* **Absolute tolerance** is necessary when comparing values near zero, where relative tolerance becomes meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_near_zero_values():\n",
    "    result = 1e-10 - 1e-10\n",
    "    # Use absolute tolerance for values near zero\n",
    "    assert abs(result - 0.0) < 1e-9\n",
    "\n",
    "test_near_zero_values()\n",
    "print(\"test_near_zero_values passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641163a",
   "metadata": {},
   "source": [
    "### Testing NumPy Arrays\n",
    "\n",
    "For NumPy arrays, you can check element-wise with a loop, but NumPy provides dedicated testing utilities in `np.testing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb780ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_array_with_numpy():\n",
    "    result = np.array([1.0, 2.0, 3.0])\n",
    "    expected = np.array([1.0, 2.0, 3.0000001])\n",
    "\n",
    "    # Use np.allclose for comparing arrays with tolerance\n",
    "    assert np.allclose(result, expected)\n",
    "\n",
    "test_array_with_numpy()\n",
    "print(\"test_array_with_numpy passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e939d",
   "metadata": {},
   "source": [
    "NumPy provides convenient functions for comparing arrays:\n",
    "\n",
    "* `np.allclose(a, b)` - check if all elements are close within tolerance\n",
    "* `np.isclose(a, b)` - element-wise comparison returning boolean array\n",
    "\n",
    "### Comparing Array Properties\n",
    "\n",
    "Sometimes you care about properties of arrays rather than exact values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def test_normalize_produces_unit_norm():\n",
    "    v = np.array([3.0, 4.0])\n",
    "    result = v / np.linalg.norm(v)  # Normalize to unit vector\n",
    "\n",
    "    # Check the norm is 1.0 (within tolerance)\n",
    "    assert math.isclose(np.linalg.norm(result), 1.0)\n",
    "\n",
    "    # Check the direction is preserved\n",
    "    assert math.isclose(result[0] / result[1], v[0] / v[1])\n",
    "\n",
    "def test_covariance_matrix_is_symmetric():\n",
    "    np.random.seed(42)\n",
    "    data = np.random.randn(100, 5)\n",
    "    cov = compute_covariance(data)\n",
    "\n",
    "    assert np.allclose(cov, cov.T)\n",
    "\n",
    "def test_output_shape_matches_input():\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 10)\n",
    "    result = transform(X)\n",
    "\n",
    "    assert result.shape == X.shape\n",
    "\n",
    "# Run all tests\n",
    "test_normalize_produces_unit_norm()\n",
    "test_covariance_matrix_is_symmetric()\n",
    "test_output_shape_matches_input()\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59438ab",
   "metadata": {},
   "source": [
    "## Reproducibility in Scientific Testing\n",
    "\n",
    "Scientific code often involves randomness: Monte Carlo simulations, random initializations, bootstrap sampling. Testing such code requires careful handling to get reproducible results.\n",
    "\n",
    "### Setting Random Seeds\n",
    "\n",
    "NumPy's random functions draw from a global random state. Setting a seed ensures the same sequence of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aeb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "print(np.random.randn(3))  # Always produces the same values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe560f",
   "metadata": {},
   "source": [
    "In tests, set the seed at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def test_bootstrap_mean():\n",
    "    np.random.seed(12345)\n",
    "    data = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    result = bootstrap_mean(data, n_iterations=1000)\n",
    "\n",
    "    # With seed fixed, we know the expected result\n",
    "    assert math.isclose(result, 3.0, rel_tol=0.05)\n",
    "\n",
    "test_bootstrap_mean()\n",
    "print(\"test_bootstrap_mean passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9834f5d",
   "metadata": {},
   "source": [
    "### Using RandomState for Isolation\n",
    "\n",
    "The global `np.random.seed()` affects all random calls, which can cause problems if functions internally use randomness. For better isolation, pass a `RandomState` object to functions that need randomness.\n",
    "\n",
    "Look back at the `bootstrap_sample` function we defined in the helper functions cell. It accepts an optional `rng` parameter that lets callers control the random state:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d1c06",
   "metadata": {},
   "source": [
    "```python\n",
    "def bootstrap_sample(data, rng=None):\n",
    "    \"\"\"Draw a bootstrap sample from data.\n",
    "\n",
    "    Args:\n",
    "        data: Input array.\n",
    "        rng: Random state for reproducibility. If None, uses global state.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState()\n",
    "    indices = rng.randint(0, len(data), size=len(data))\n",
    "    return data[indices]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d9ed9",
   "metadata": {},
   "source": [
    "In tests, pass an explicit random state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bootstrap_sample_reproducibility():\n",
    "    data = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "    rng1 = np.random.RandomState(42)\n",
    "    rng2 = np.random.RandomState(42)\n",
    "\n",
    "    sample1 = bootstrap_sample(data, rng=rng1)\n",
    "    sample2 = bootstrap_sample(data, rng=rng2)\n",
    "\n",
    "    assert np.allclose(sample1, sample2)\n",
    "\n",
    "test_bootstrap_sample_reproducibility()\n",
    "print(\"test_bootstrap_sample_reproducibility passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba72258",
   "metadata": {},
   "source": [
    "### Testing Statistical Properties\n",
    "\n",
    "For stochastic algorithms, test statistical properties rather than exact values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d541df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_standard_normal(n, seed=None):\n",
    "    \"\"\"Generate n samples approximating standard normal distribution.\n",
    "    \n",
    "    Uses Box-Muller transform to convert uniform random numbers\n",
    "    to normally distributed values.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    u1 = np.random.random(n)\n",
    "    u2 = np.random.random(n)\n",
    "    return np.sqrt(-2 * np.log(u1)) * np.cos(2 * np.pi * u2)\n",
    "\n",
    "def test_generated_samples_have_correct_mean():\n",
    "    samples = generate_standard_normal(10000, seed=42)\n",
    "    # Mean should be close to 0 for large samples\n",
    "    assert abs(np.mean(samples) - 0.0) < 0.05\n",
    "\n",
    "def test_generated_samples_have_correct_std():\n",
    "    samples = generate_standard_normal(10000, seed=42)\n",
    "    # Std should be close to 1 for large samples\n",
    "    assert abs(np.std(samples) - 1.0) < 0.05\n",
    "\n",
    "# Run tests\n",
    "test_generated_samples_have_correct_mean()\n",
    "test_generated_samples_have_correct_std()\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d85b5",
   "metadata": {},
   "source": [
    "The tolerances must account for expected statistical variation.\n",
    "\n",
    "### Testing Edge Cases\n",
    "\n",
    "Edge cases are critical in numerical code. Test boundaries and unusual inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f569252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_single_element():\n",
    "    result = calculate_mean(np.array([42.0]))\n",
    "    assert result == 42.0\n",
    "\n",
    "def test_variance_constant_array():\n",
    "    result = calculate_variance(np.array([5.0, 5.0, 5.0, 5.0]))\n",
    "    assert result == 0.0\n",
    "\n",
    "def test_normalize_handles_large_values():\n",
    "    data = np.array([1e10, 2e10, 3e10])\n",
    "    result = normalize(data)\n",
    "    assert np.isfinite(result).all()\n",
    "\n",
    "# Run tests\n",
    "test_mean_single_element()\n",
    "test_variance_constant_array()\n",
    "test_normalize_handles_large_values()\n",
    "print(\"All edge case tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cec3f8",
   "metadata": {},
   "source": [
    "Testing for expected errors (like checking that a function raises an exception for invalid input) is also possible but requires more advanced Python concepts. For now, focus on testing that your functions return correct results for valid inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa96bb",
   "metadata": {},
   "source": [
    "## Scaling Up with pytest\n",
    "\n",
    "The manual testing approach shown above works well for learning and small projects. As your test suite grows, you'll encounter challenges:\n",
    "\n",
    "* Manually calling each test function becomes tedious\n",
    "* When a test fails, you have to re-run the entire file\n",
    "* There's no summary of which tests passed or failed\n",
    "\n",
    "**pytest** is a testing framework that solves these problems. It automatically discovers and runs tests, provides detailed failure reports, and offers advanced features.\n",
    "\n",
    "### Installing and Using pytest\n",
    "\n",
    "Install pytest with pip:\n",
    "```bash\n",
    "pip install pytest\n",
    "```\n",
    "\n",
    "pytest tests look almost identical to manual tests - the main difference is you don't call the test functions yourself:\n",
    "\n",
    "```python\n",
    "# test_math_utils.py\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add_positive_numbers():\n",
    "    assert add(2, 3) == 5\n",
    "\n",
    "def test_add_negative_numbers():\n",
    "    assert add(-1, -1) == -2\n",
    "\n",
    "# No need to call test functions - pytest does it automatically\n",
    "```\n",
    "\n",
    "Run pytest from the command line:\n",
    "```bash\n",
    "pytest              # Run all tests\n",
    "pytest -v           # Verbose output showing each test\n",
    "pytest test_file.py # Run tests in specific file\n",
    "```\n",
    "\n",
    "pytest automatically discovers all files named `test_*.py` and runs all functions named `test_*`.\n",
    "\n",
    "For more details on pytest's features like fixtures and parameterization, see the [pytest documentation](https://docs.pytest.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c5cad",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### Test Isolation and Independence\n",
    "\n",
    "Each test should be independent. Tests should not:\n",
    "* Depend on the order they run in\n",
    "* Share state with other tests\n",
    "* Require other tests to have run first\n",
    "* Leave side effects that affect other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fb633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: Tests share state\n",
    "counter = 0\n",
    "\n",
    "def test_increment():\n",
    "    global counter\n",
    "    counter += 1\n",
    "    assert counter == 1\n",
    "\n",
    "def test_counter_value():\n",
    "    assert counter == 1  # Depends on test_increment running first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd194f",
   "metadata": {},
   "source": [
    "### Test Coverage: Quality Over Quantity\n",
    "\n",
    "Test coverage measures what percentage of your code is executed by tests. While high coverage is good, it's not sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09205aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a, b):\n",
    "    return a / b\n",
    "\n",
    "def test_divide():\n",
    "    assert divide(10, 2) == 5  # 100% coverage, but misses edge case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5a4c2",
   "metadata": {},
   "source": [
    "This test achieves 100% code coverage but doesn't test division by zero. Focus on testing:\n",
    "* Normal cases (typical inputs)\n",
    "* Edge cases (empty inputs, single elements, boundaries)\n",
    "* Error cases (invalid inputs, exceptions)\n",
    "* Corner cases specific to your domain (numerical stability, overflow)\n",
    "\n",
    "### When to Write Tests\n",
    "\n",
    "There are different philosophies about when to write tests:\n",
    "\n",
    "**Test-Driven Development (TDD):** Write tests before code. This forces you to think about the interface before implementation.\n",
    "\n",
    "**Test-After:** Write code first, then tests. This is more common in exploratory scientific work.\n",
    "\n",
    "**Test-During:** Write tests alongside code, alternating between implementation and testing.\n",
    "\n",
    "For scientific computing, a practical approach is:\n",
    "1. Prototype without tests for exploration\n",
    "2. Add tests once the interface stabilizes\n",
    "3. Always add a test when you find a bug (regression test)\n",
    "\n",
    "### Continuous Integration\n",
    "\n",
    "Tests are most valuable when run automatically. Continuous Integration (CI) services run your tests whenever you push code changes. Popular options include GitHub Actions, GitLab CI, and Travis CI.\n",
    "\n",
    "A typical CI workflow:\n",
    "1. Developer pushes code to repository\n",
    "2. CI server detects the push\n",
    "3. CI runs all tests\n",
    "4. Developer is notified if any tests fail\n",
    "\n",
    "This catches bugs early and ensures tests are always run, not just when developers remember to run them locally.\n",
    "\n",
    "### Test File Organization\n",
    "\n",
    "For larger projects, keep test files separate from your source code:\n",
    "\n",
    "```\n",
    "myproject/\n",
    "    mypackage/\n",
    "        stats.py\n",
    "        models.py\n",
    "    tests/\n",
    "        test_stats.py\n",
    "        test_models.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67d2ee",
   "metadata": {},
   "source": [
    "## Recommended Resources\n",
    "\n",
    "* [pytest documentation](https://docs.pytest.org/) - Official pytest documentation\n",
    "* [Real Python - Effective Python Testing With pytest](https://realpython.com/pytest-python-testing/) - Comprehensive pytest tutorial\n",
    "* [Scientific Python Development Guide - Testing](https://learn.scientific-python.org/development/tutorials/test/) - Testing practices for scientific Python\n",
    "* [NumPy Testing Guidelines](https://numpy.org/doc/stable/reference/routines.testing.html) - NumPy's testing utilities"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
